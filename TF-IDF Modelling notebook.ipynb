{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaeyoonlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaeyoonlee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaeyoonlee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaeyoonlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>work_type</th>\n",
       "      <th>location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>industry</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>description_length</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Validation Engineer, Labware LIMS</td>\n",
       "      <td>Validation Engineer, Labware LIMSFoster City, ...</td>\n",
       "      <td>I.T. Solutions, Inc.</td>\n",
       "      <td>135200.0</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>Foster City, CA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2084</td>\n",
       "      <td>validation engineer labware limsfoster city va...</td>\n",
       "      <td>validation engineer labware lims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Administrative Assistant - CONCUR</td>\n",
       "      <td>Global Financial Services firm is seeking an e...</td>\n",
       "      <td>ActOne Group</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Information Services</td>\n",
       "      <td>Associate</td>\n",
       "      <td>1046</td>\n",
       "      <td>global financial service firm seek experienced...</td>\n",
       "      <td>administrative assistant concur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer Service Representative</td>\n",
       "      <td>We are seeking future agents to join our team!...</td>\n",
       "      <td>ABC Farigua Division</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Greater Orlando, FL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>1402</td>\n",
       "      <td>seek future agent join team look driven self-m...</td>\n",
       "      <td>customer service representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inbound Call Center Specialist</td>\n",
       "      <td>Always Connecting, Always Evolving.\\nIf you ar...</td>\n",
       "      <td>TECHEAD</td>\n",
       "      <td>38480.0</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>Richmond, VA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>Associate</td>\n",
       "      <td>3077</td>\n",
       "      <td>always connect always evolve look new opportun...</td>\n",
       "      <td>inbound call center specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tool and Die Maker</td>\n",
       "      <td>Job Summary:The Tool and Die Maker will build ...</td>\n",
       "      <td>Prolink</td>\n",
       "      <td>69680.0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>Cincinnati Metropolitan Area</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>Associate</td>\n",
       "      <td>1067</td>\n",
       "      <td>job summary tool die maker build dy concept st...</td>\n",
       "      <td>tool die maker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              title  \\\n",
       "0           0  Validation Engineer, Labware LIMS   \n",
       "1           1  Administrative Assistant - CONCUR   \n",
       "2           2    Customer Service Representative   \n",
       "3           3     Inbound Call Center Specialist   \n",
       "4           4                 Tool and Die Maker   \n",
       "\n",
       "                                         description               company  \\\n",
       "0  Validation Engineer, Labware LIMSFoster City, ...  I.T. Solutions, Inc.   \n",
       "1  Global Financial Services firm is seeking an e...          ActOne Group   \n",
       "2  We are seeking future agents to join our team!...  ABC Farigua Division   \n",
       "3  Always Connecting, Always Evolving.\\nIf you ar...               TECHEAD   \n",
       "4  Job Summary:The Tool and Die Maker will build ...               Prolink   \n",
       "\n",
       "     salary  work_type                      location  company_size  \\\n",
       "0  135200.0   CONTRACT               Foster City, CA           3.0   \n",
       "1   82500.0  FULL_TIME                  New York, NY           5.0   \n",
       "2   90000.0  FULL_TIME           Greater Orlando, FL           3.0   \n",
       "3   38480.0   CONTRACT                  Richmond, VA           2.0   \n",
       "4   69680.0  FULL_TIME  Cincinnati Metropolitan Area           4.0   \n",
       "\n",
       "                        industry  experience_level  description_length  \\\n",
       "0  IT Services and IT Consulting  Mid-Senior level                2084   \n",
       "1           Information Services         Associate                1046   \n",
       "2                      Insurance       Entry level                1402   \n",
       "3        Staffing and Recruiting         Associate                3077   \n",
       "4        Staffing and Recruiting         Associate                1067   \n",
       "\n",
       "                                 cleaned_description  \\\n",
       "0  validation engineer labware limsfoster city va...   \n",
       "1  global financial service firm seek experienced...   \n",
       "2  seek future agent join team look driven self-m...   \n",
       "3  always connect always evolve look new opportun...   \n",
       "4  job summary tool die maker build dy concept st...   \n",
       "\n",
       "                      cleaned_title  \n",
       "0  validation engineer labware lims  \n",
       "1   administrative assistant concur  \n",
       "2   customer service representative  \n",
       "3    inbound call center specialist  \n",
       "4                    tool die maker  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"C:/Users/fairt/OneDrive/Desktop/Text Analytic/Project/cleaned_job_postings.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             0\n",
      "title                  0\n",
      "description            0\n",
      "company                0\n",
      "salary                 0\n",
      "work_type              0\n",
      "location               0\n",
      "company_size           0\n",
      "industry               0\n",
      "experience_level       0\n",
      "description_length     0\n",
      "cleaned_description    0\n",
      "cleaned_title          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['cleaned_title'])\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Initialize Skill Categories and Stopwords\n",
      "---------------------------------------------\n",
      "Initialized skill categories: ['Creative Arts', 'Digital Design', 'Software Development', 'Marketing', 'Project Management', 'Product Management', 'Data Analysis', 'Business Analysis', 'Financial', 'Sales']\n",
      "\n",
      "Total skill-related terms in vocabulary: 156\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create Custom Stopwords and Initialize Skill Categories\n",
    "print(\"\\nStep 4: Initialize Skill Categories and Stopwords\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Add high-frequency but low-predictive words to stopwords\n",
    "additional_stopwords = [\n",
    "    'work', 'experience', 'team', 'include', 'service', 'customer',\n",
    "    'provide', 'skill', 'year', 'job', 'support', 'opportunity',\n",
    "    'position', 'business', 'employee', 'company', 'benefit',\n",
    "    'require', 'ability', 'candidate', 'requirement', 'qualified',\n",
    "    'responsibilities', 'duties', 'role',\n",
    "    # Adding missing words from the second list\n",
    "    'years', 'including', 'must', 'will', 'new', 'looking', \n",
    "    'seeking', 'ideal', 'competitive', 'excellent', 'strong', \n",
    "    'great', 'minimum', 'preferred', 'qualification', 'knowledge', \n",
    "    'related', 'professional', 'proficiency', 'based', 'remote', \n",
    "    'hybrid', 'office', 'requirements', 'skills'\n",
    "]\n",
    "# Combine with standard stopwords - convert to list\n",
    "custom_stopwords = list(set(stopwords.words('english')).union(additional_stopwords))\n",
    "\n",
    "skill_categories = {\n",
    "    'Creative Arts': {\n",
    "        'keywords': [\n",
    "            'adobe creative suite', 'photoshop', 'illustrator', 'indesign',\n",
    "            'graphic design', 'visual design', 'typography', 'art direction',\n",
    "            'creative direction', 'brand design', 'illustration', 'adobe xd',\n",
    "            'figma', 'sketch', 'color theory', 'layout design'\n",
    "        ],\n",
    "        'context_required': ['design', 'creative', 'art', 'visual']\n",
    "    },\n",
    "    'Digital Design': {\n",
    "        'keywords': [\n",
    "            'ui design', 'ux design', 'user interface', 'user experience',\n",
    "            'wireframing', 'prototyping', 'responsive design', 'mobile design',\n",
    "            'web design', 'interaction design', 'usability testing',\n",
    "            'information architecture', 'figma', 'sketch'\n",
    "        ],\n",
    "        'context_required': ['design', 'user', 'interface', 'experience']\n",
    "    },\n",
    "    'Software Development': {\n",
    "        'keywords': [\n",
    "            'java', 'python', 'javascript', 'react', 'angular', 'node.js',\n",
    "            'full stack', 'front end', 'back end', 'web development',\n",
    "            'api development', 'cloud computing', r'\\baws\\b', 'azure',\n",
    "            'devops', 'ci/cd', 'docker', 'kubernetes'\n",
    "        ],\n",
    "        'context_required': ['development', 'programming', 'software']\n",
    "    },\n",
    "    'Marketing': {\n",
    "        'keywords': [\n",
    "            'digital marketing', 'content marketing', r'\\bseo\\b', r'\\bsem\\b',\n",
    "            'social media marketing', 'email marketing', 'marketing automation',\n",
    "            'google analytics', 'conversion optimization', 'brand marketing',\n",
    "            'marketing strategy', 'campaign management', 'hubspot', 'marketo'\n",
    "        ],\n",
    "        'context_required': ['marketing', 'digital']\n",
    "    },\n",
    "    'Project Management': {\n",
    "        'keywords': [\n",
    "            'project management', 'agile methodology', 'scrum master',\n",
    "            'project planning', 'risk management', 'stakeholder management',\n",
    "            'pmp certification', 'project coordination', 'jira', 'asana',\n",
    "            'microsoft project', 'project lifecycle', 'change management'\n",
    "        ],\n",
    "        'context_required': ['project', 'management']\n",
    "    },\n",
    "    'Product Management': {\n",
    "        'keywords': [\n",
    "            'product strategy', 'product roadmap', 'product development',\n",
    "            'product lifecycle', 'agile product', 'product owner', 'scrum',\n",
    "            'market research', 'user stories', 'feature prioritization',\n",
    "            'product metrics', 'product analytics', 'product launch'\n",
    "        ],\n",
    "        'context_required': ['product']\n",
    "    },\n",
    "    'Data Analysis': {\n",
    "        'keywords': [\n",
    "            'data analysis', 'statistical analysis', 'data visualization',\n",
    "            'sql', 'python', 'r programming', 'tableau', 'power bi',\n",
    "            'excel advanced', 'data modeling', 'regression analysis',\n",
    "            'hypothesis testing', 'a/b testing', 'data mining'\n",
    "        ],\n",
    "        'context_required': ['data', 'analysis', 'analytics']\n",
    "    },\n",
    "\n",
    "    'Business Analysis': {\n",
    "        'keywords': [\n",
    "            'business analysis', 'requirements gathering', 'process mapping',\n",
    "            'gap analysis', 'business process', 'system analysis',\n",
    "            'functional requirements', 'business intelligence', 'data modeling',\n",
    "            'process improvement', 'workflow optimization'\n",
    "        ],\n",
    "        'context_required': ['analysis', 'business']\n",
    "    },\n",
    "    'Financial': {\n",
    "        'keywords': [\n",
    "            'financial analysis', 'financial modeling', 'forecasting',\n",
    "            'budgeting', 'variance analysis', 'cost analysis', 'pricing',\n",
    "            'profit and loss', 'balance sheet', 'financial reporting',\n",
    "            'risk assessment', 'investment analysis'\n",
    "        ],\n",
    "        'context_required': ['financial', 'finance']\n",
    "    },\n",
    "    'Sales': {\n",
    "        'keywords': [\n",
    "            'sales strategy', 'account management', 'sales forecasting',\n",
    "            'crm', 'salesforce', 'sales operations', 'business development',\n",
    "            'lead generation', 'pipeline management', 'contract negotiation',\n",
    "            'sales analytics', 'territory management'\n",
    "        ],\n",
    "        'context_required': ['sales', 'revenue']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Initialized skill categories:\", list(skill_categories.keys()))\n",
    "\n",
    "# Create vocabulary from skill categories\n",
    "skill_vocabulary = set()\n",
    "for category in skill_categories.values():\n",
    "    skill_vocabulary.update(category['keywords'])\n",
    "    skill_vocabulary.update(category['context_required'])\n",
    "\n",
    "print(f\"\\nTotal skill-related terms in vocabulary: {len(skill_vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "Tuning TF-IDF parameters...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Processing full dataset with tuned parameters...\n",
      "\n",
      "Plotting TF-IDF word frequency distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fairt\\AppData\\Local\\Temp\\ipykernel_24184\\2543986868.py:103: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=top_terms, x='score', y='term', palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "Model Performance:\n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error: $27138.60\n",
      "Mean Absolute Error: $20982.52\n",
      "R-squared Score: 0.592\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "--------------------------------------------------\n",
      "                        feature  importance\n",
      "5540   experience_level_encoded    0.026007\n",
      "5544               exp_industry    0.014401\n",
      "4561               tfidf_senior    0.013939\n",
      "1183     tfidf_customer service    0.011960\n",
      "4513       tfidf_school diploma    0.011654\n",
      "1348              tfidf_develop    0.011347\n",
      "2311          tfidf_high school    0.010397\n",
      "1326               tfidf_design    0.010373\n",
      "1686          tfidf_engineering    0.010032\n",
      "1684             tfidf_engineer    0.009747\n",
      "4512               tfidf_school    0.009735\n",
      "1384              tfidf_diploma    0.009634\n",
      "2312  tfidf_high school diploma    0.009161\n",
      "2819                 tfidf_lift    0.007560\n",
      "3881              tfidf_project    0.007423\n",
      "\n",
      "Model Improvement:\n",
      "--------------------------------------------------\n",
      "Baseline RMSE (mean prediction): $42512.88\n",
      "Model RMSE: $27138.60\n",
      "Improvement over baseline: 36.2%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.best_tfidf = None\n",
    "        self.best_rf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def create_tfidf_features(self, df):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features from text data using Ridge regression for faster tuning\n",
    "        \"\"\"\n",
    "        # Combine text features\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Create TF-IDF Pipeline with Ridge\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('ridge', Ridge(random_state=42, alpha=1.0))  # Using Ridge instead of RF\n",
    "        ])\n",
    "        \n",
    "        # Parameter grid for TF-IDF only\n",
    "        param_grid = {\n",
    "            'tfidf__ngram_range': [(1,2),(1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.05, 0.1, 0.2],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False]\n",
    "        }\n",
    "        \n",
    "        # Tune TF-IDF parameters\n",
    "        print(\"Tuning TF-IDF parameters...\")\n",
    "        grid = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid.fit(combined_text, df['salary'])\n",
    "        \n",
    "        # Process full dataset\n",
    "        print(\"\\nProcessing full dataset with tuned parameters...\")\n",
    "        combined_text_full = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Generate TF-IDF features\n",
    "        self.best_tfidf = grid.best_estimator_.named_steps['tfidf']\n",
    "        tfidf_matrix = self.best_tfidf.fit_transform(combined_text_full)\n",
    "        tfidf_features = pd.DataFrame(\n",
    "            tfidf_matrix.toarray(),\n",
    "            columns=[f'tfidf_{f}' for f in self.best_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_df = tfidf_features.copy()\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['experience_level_encoded'] = self.label_encoder.fit_transform(df['experience_level'])\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'])\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'])\n",
    "        \n",
    "        # Add location features\n",
    "        feature_df['state'] = df['location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "        feature_df['state_encoded'] = self.label_encoder.fit_transform(feature_df['state'])\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_df['exp_industry'] = feature_df['experience_level_encoded'] * feature_df['industry_encoded']\n",
    "        \n",
    "        return feature_df, tfidf_matrix\n",
    "\n",
    "    def plot_tfidf_distribution(self, tfidf_matrix, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot the TF-IDF word frequency distribution\n",
    "        \"\"\"\n",
    "        # Calculate mean TF-IDF scores for each term\n",
    "        mean_tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = self.best_tfidf.get_feature_names_out()\n",
    "        \n",
    "        # Create DataFrame with terms and their scores\n",
    "        tfidf_dist_df = pd.DataFrame({\n",
    "            'term': feature_names,\n",
    "            'score': mean_tfidf_scores\n",
    "        })\n",
    "        \n",
    "        # Sort by score and get top 30 terms\n",
    "        top_terms = tfidf_dist_df.nlargest(30, 'score')\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.barplot(data=top_terms, x='score', y='term', palette='viridis')\n",
    "        \n",
    "        plt.title('Top 30 Terms by Mean TF-IDF Score', fontsize=14, pad=20)\n",
    "        plt.xlabel('Mean TF-IDF Score', fontsize=12)\n",
    "        plt.ylabel('Term', fontsize=12)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def train_model(self, feature_matrix, target):\n",
    "        \"\"\"\n",
    "        Train the final RandomForest model\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        X = feature_matrix.drop(['state'], axis=1)\n",
    "        y = target\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize and train the model with best parameters\n",
    "        self.best_rf = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.best_rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions and evaluate\n",
    "        y_pred = self.best_rf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'feature_importance': feature_importance,\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Save all model components\n",
    "        \"\"\"\n",
    "        joblib.dump(self.best_tfidf, f'{path_prefix}tfidf.joblib')\n",
    "        joblib.dump(self.best_rf, f'{path_prefix}random_forest.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Load all model components\n",
    "        \"\"\"\n",
    "        self.best_tfidf = joblib.load(f'{path_prefix}tfidf.joblib')\n",
    "        self.best_rf = joblib.load(f'{path_prefix}random_forest.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Create features\n",
    "    print(\"Creating TF-IDF features...\")\n",
    "    feature_matrix, tfidf_matrix = model.create_tfidf_features(df)\n",
    "    \n",
    "    # Plot TF-IDF distribution\n",
    "    print(\"\\nPlotting TF-IDF word frequency distribution...\")\n",
    "    model.plot_tfidf_distribution(tfidf_matrix, save_path='tfidf_distribution.png')\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Root Mean Squared Error: ${results['rmse']:.2f}\")\n",
    "    print(f\"Mean Absolute Error: ${results['mae']:.2f}\")\n",
    "    print(f\"R-squared Score: {results['r2']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results['feature_importance'].head(15).to_string())\n",
    "    \n",
    "    # Calculate percentage improvement over baseline\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(results['y_test'], \n",
    "                                             [results['y_test'].mean()] * len(results['y_test'])))\n",
    "    improvement = ((baseline_rmse - results['rmse']) / baseline_rmse) * 100\n",
    "    \n",
    "    print(f\"\\nModel Improvement:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Baseline RMSE (mean prediction): ${baseline_rmse:.2f}\")\n",
    "    print(f\"Model RMSE: ${results['rmse']:.2f}\")\n",
    "    print(f\"Improvement over baseline: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Random Forest with skill categories BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning general TF-IDF parameters...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Processing full dataset with tuned parameters...\n",
      "\n",
      "Model Performance:\n",
      "Root Mean Squared Error: $26377.55\n",
      "Mean Absolute Error: $20181.65\n",
      "R-squared Score: 0.615\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                       feature  importance\n",
      "2809  experience_level_encoded    0.021663\n",
      "2813              exp_industry    0.018380\n",
      "1127       general_high school    0.017697\n",
      "665            general_diploma    0.013972\n",
      "2142    general_school diploma    0.011931\n",
      "2168            general_senior    0.011393\n",
      "570   general_customer service    0.011007\n",
      "2141            general_school    0.010748\n",
      "815           general_engineer    0.010304\n",
      "634             general_design    0.010099\n",
      "2312         general_strategic    0.009397\n",
      "648            general_develop    0.009340\n",
      "816        general_engineering    0.009088\n",
      "737               general_duty    0.008476\n",
      "1368              general_lift    0.008129\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.best_general_tfidf = None\n",
    "        self.best_skill_tfidf = None\n",
    "        self.best_rf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def create_dual_tfidf_features(self, df, skill_categories):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features from both general text and skill-specific vocabulary\n",
    "        \"\"\"\n",
    "        # Sample subset for initial tuning\n",
    "        sample_size = int(len(df) * 0.2)\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Combine text features\n",
    "        combined_text = df_sample['cleaned_title'] + ' ' + df_sample['cleaned_description']\n",
    "        \n",
    "        # 1. General TF-IDF Pipeline\n",
    "        general_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # 2. Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        # Parameter grids for both TF-IDF vectorizers\n",
    "        param_grid = {\n",
    "            # TF-IDF parameters\n",
    "            'tfidf__ngram_range': [(1, 2),(1,3)],\n",
    "            'tfidf__min_df': [0.01, 0.02],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False],\n",
    "            # RandomForest parameters\n",
    "            'rf__n_estimators': [100, 200],\n",
    "            'rf__max_depth': [10, 20],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # Tune general TF-IDF\n",
    "        print(\"Tuning general TF-IDF parameters...\")\n",
    "        general_grid = GridSearchCV(\n",
    "            general_pipeline,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        general_grid.fit(combined_text, df_sample['salary'])\n",
    "        \n",
    "        # Create skill-specific pipeline with best general parameters\n",
    "        best_params = general_grid.best_params_\n",
    "        skill_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                vocabulary=list(skill_vocabulary),\n",
    "                stop_words='english',\n",
    "                ngram_range=best_params['tfidf__ngram_range'],\n",
    "                min_df=best_params['tfidf__min_df'],\n",
    "                max_df=best_params['tfidf__max_df'],\n",
    "                binary=best_params['tfidf__binary']\n",
    "            )),\n",
    "            ('rf', RandomForestRegressor(\n",
    "                n_estimators=best_params['rf__n_estimators'],\n",
    "                max_depth=best_params['rf__max_depth'],\n",
    "                min_samples_split=best_params['rf__min_samples_split'],\n",
    "                min_samples_leaf=best_params['rf__min_samples_leaf'],\n",
    "                max_features=best_params['rf__max_features'],\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Process full dataset\n",
    "        print(\"\\nProcessing full dataset with tuned parameters...\")\n",
    "        combined_text_full = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Generate general TF-IDF features\n",
    "        self.best_general_tfidf = general_grid.best_estimator_.named_steps['tfidf']\n",
    "        general_matrix = self.best_general_tfidf.fit_transform(combined_text_full)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.best_general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Generate skill-specific TF-IDF features\n",
    "        self.best_skill_tfidf = skill_pipeline.named_steps['tfidf']\n",
    "        skill_matrix = self.best_skill_tfidf.fit_transform(combined_text_full)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.best_skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        feature_df = pd.concat([general_features, skill_features], axis=1)\n",
    "        \n",
    "        # Add category-level aggregated features\n",
    "        for category, info in skill_categories.items():\n",
    "            keyword_cols = [col for col in skill_features.columns \n",
    "                          if any(keyword in col for keyword in info['keywords'])]\n",
    "            context_cols = [col for col in skill_features.columns \n",
    "                          if any(context in col for context in info['context_required'])]\n",
    "            \n",
    "            if keyword_cols:\n",
    "                feature_df[f'{category}_keyword_score'] = skill_features[keyword_cols].sum(axis=1) * 2\n",
    "            if context_cols:\n",
    "                feature_df[f'{category}_context_score'] = skill_features[context_cols].sum(axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['experience_level_encoded'] = self.label_encoder.fit_transform(df['experience_level'])\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'])\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'])\n",
    "        \n",
    "        # Add location features\n",
    "        feature_df['state'] = df['location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "        feature_df['state_encoded'] = self.label_encoder.fit_transform(feature_df['state'])\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_df['exp_industry'] = feature_df['experience_level_encoded'] * feature_df['industry_encoded']\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def train_model(self, feature_matrix, target):\n",
    "        \"\"\"\n",
    "        Train the final model using the best parameters\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        X = feature_matrix.drop(['state'], axis=1)\n",
    "        y = target\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize and train the model with best parameters\n",
    "        self.best_rf = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.best_rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions and evaluate\n",
    "        y_pred = self.best_rf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'feature_importance': feature_importance,\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Save all model components\n",
    "        \"\"\"\n",
    "        joblib.dump(self.best_general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.best_skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.best_rf, f'{path_prefix}random_forest.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Load all model components\n",
    "        \"\"\"\n",
    "        self.best_general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.best_skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.best_rf = joblib.load(f'{path_prefix}random_forest.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Assuming you have your data and skill categories ready\n",
    "    # df = your_dataframe\n",
    "    # skill_categories = your_skill_categories\n",
    "    \n",
    "    # Create features\n",
    "    feature_matrix = model.create_dual_tfidf_features(df, skill_categories)\n",
    "    \n",
    "    # Train model\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Root Mean Squared Error: ${results['rmse']:.2f}\")\n",
    "    print(f\"Mean Absolute Error: ${results['mae']:.2f}\")\n",
    "    print(f\"R-squared Score: {results['r2']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(results['feature_importance'].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Random Forest with Separate Hyperparameter Tuning for general/skill-specific parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning general TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.457\n",
      "Standard Deviation: 0.010\n",
      "\n",
      "Tuning skill-specific TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Skill TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.288\n",
      "Standard Deviation: 0.025\n",
      "\n",
      "Processing full dataset with tuned parameters...\n",
      "\n",
      "Best General TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': False, 'tfidf__max_df': 0.9, 'tfidf__min_df': 0.05, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Best Skill TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': True, 'tfidf__max_df': 0.8, 'tfidf__min_df': 0.01, 'tfidf__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "Model Performance:\n",
      "Mean R-squared across folds: 0.623\n",
      "Standard Deviation of R-squared: 0.006\n",
      "Mean RMSE across folds: $26126.36\n",
      "Standard Deviation of RMSE: $201.90\n",
      "\n",
      "Best Parameters:\n",
      "{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                       feature  importance\n",
      "1368  experience_level_encoded    0.033304\n",
      "1372              exp_industry    0.020192\n",
      "1259          skill_experience    0.017538\n",
      "250   general_customer service    0.015062\n",
      "977     general_school diploma    0.014708\n",
      "287            general_diploma    0.014546\n",
      "976             general_school    0.014412\n",
      "990             general_senior    0.012998\n",
      "493        general_high school    0.011457\n",
      "357        general_engineering    0.011060\n",
      "356           general_engineer    0.010278\n",
      "277             general_design    0.009909\n",
      "1050         general_strategic    0.009021\n",
      "603               general_lift    0.008821\n",
      "281            general_develop    0.008797\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.best_general_tfidf = None\n",
    "        self.best_skill_tfidf = None\n",
    "        self.best_rf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    def create_dual_tfidf_features(self, df, skill_categories):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features from both general text and skill-specific vocabulary with separate grid searches\n",
    "        and cross validation using R-squared as the scoring metric\n",
    "        \"\"\"\n",
    "        # Sample subset for initial tuning\n",
    "        sample_size = int(len(df) * 0.2)\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Combine text features\n",
    "        combined_text = df_sample['cleaned_title'] + ' ' + df_sample['cleaned_description']\n",
    "        \n",
    "        # 1. General TF-IDF Pipeline\n",
    "        general_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=\"english\")),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # General TF-IDF parameter grid\n",
    "        general_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.05],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # 2. Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        # Skill-specific TF-IDF Pipeline\n",
    "        skill_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                vocabulary=list(skill_vocabulary),\n",
    "                stop_words = \"english\"\n",
    "            )),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # Skill-specific parameter grid\n",
    "        skill_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.03],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # Tune general TF-IDF with cross validation using R-squared\n",
    "        print(\"Tuning general TF-IDF parameters...\")\n",
    "        general_grid = GridSearchCV(\n",
    "            general_pipeline,\n",
    "            general_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'  # Changed to R-squared\n",
    "        )\n",
    "        general_grid.fit(combined_text, df_sample['salary'])\n",
    "        \n",
    "        # Print cross validation results for general TF-IDF\n",
    "        print(\"\\nGeneral TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(general_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {general_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[general_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Tune skill-specific TF-IDF with cross validation\n",
    "        print(\"\\nTuning skill-specific TF-IDF parameters...\")\n",
    "        skill_grid = GridSearchCV(\n",
    "            skill_pipeline,\n",
    "            skill_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'  # Changed to R-squared\n",
    "        )\n",
    "        skill_grid.fit(combined_text, df_sample['salary'])\n",
    "        \n",
    "        # Print cross validation results for skill TF-IDF\n",
    "        print(\"\\nSkill TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(skill_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {skill_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[skill_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Process full dataset\n",
    "        print(\"\\nProcessing full dataset with tuned parameters...\")\n",
    "        combined_text_full = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Generate general TF-IDF features\n",
    "        self.best_general_tfidf = general_grid.best_estimator_.named_steps['tfidf']\n",
    "        general_matrix = self.best_general_tfidf.fit_transform(combined_text_full)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.best_general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Generate skill-specific TF-IDF features\n",
    "        self.best_skill_tfidf = skill_grid.best_estimator_.named_steps['tfidf']\n",
    "        skill_matrix = self.best_skill_tfidf.fit_transform(combined_text_full)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.best_skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Print best parameters for both grids\n",
    "        print(\"\\nBest General TF-IDF Parameters:\")\n",
    "        print(general_grid.best_params_)\n",
    "        print(\"\\nBest Skill TF-IDF Parameters:\")\n",
    "        print(skill_grid.best_params_)\n",
    "        \n",
    "        # Combine features\n",
    "        feature_df = pd.concat([general_features, skill_features], axis=1)\n",
    "        \n",
    "        # Add category-level aggregated features\n",
    "        for category, info in skill_categories.items():\n",
    "            keyword_cols = [col for col in skill_features.columns \n",
    "                          if any(keyword in col for keyword in info['keywords'])]\n",
    "            context_cols = [col for col in skill_features.columns \n",
    "                          if any(context in col for context in info['context_required'])]\n",
    "            \n",
    "            if keyword_cols:\n",
    "                feature_df[f'{category}_keyword_score'] = skill_features[keyword_cols].sum(axis=1) * 2\n",
    "            if context_cols:\n",
    "                feature_df[f'{category}_context_score'] = skill_features[context_cols].sum(axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['experience_level_encoded'] = self.label_encoder.fit_transform(df['experience_level'])\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'])\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'])\n",
    "        \n",
    "        # Add location features\n",
    "        feature_df['state'] = df['location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "        feature_df['state_encoded'] = self.label_encoder.fit_transform(feature_df['state'])\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_df['exp_industry'] = feature_df['experience_level_encoded'] * feature_df['industry_encoded']\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def train_model(self, feature_matrix, target):\n",
    "        \"\"\"\n",
    "        Train the final model using cross validation with R-squared as the main metric\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        X = feature_matrix.drop(['state'], axis=1)\n",
    "        y = target\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        # Define parameter grid for final model\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "\n",
    "        # Perform grid search with cross validation using R-squared\n",
    "        rf_grid = GridSearchCV(\n",
    "            RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'  # Changed to R-squared\n",
    "        )\n",
    "        \n",
    "        rf_grid.fit(X_scaled, y)\n",
    "        \n",
    "        # Store best model\n",
    "        self.best_rf = rf_grid.best_estimator_\n",
    "\n",
    "        # Perform cross validation on best model\n",
    "        r2_scores = cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Also calculate RMSE for reference\n",
    "        rmse_scores = np.sqrt(-cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Final evaluation metrics\n",
    "        final_results = {\n",
    "            'cv_r2_mean': r2_scores.mean(),\n",
    "            'cv_r2_std': r2_scores.std(),\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std(),\n",
    "            'best_params': rf_grid.best_params_,\n",
    "            'feature_importance': feature_importance,\n",
    "            'cv_r2_scores': r2_scores,\n",
    "            'cv_rmse_scores': rmse_scores\n",
    "        }\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Save all model components\n",
    "        \"\"\"\n",
    "        joblib.dump(self.best_general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.best_skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.best_rf, f'{path_prefix}random_forest.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Load all model components\n",
    "        \"\"\"\n",
    "        self.best_general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.best_skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.best_rf = joblib.load(f'{path_prefix}random_forest.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Assuming you have your data and skill categories ready\n",
    "    # df = your_dataframe\n",
    "    # skill_categories = your_skill_categories\n",
    "    \n",
    "    # Create features\n",
    "    feature_matrix = model.create_dual_tfidf_features(df, skill_categories)\n",
    "    \n",
    "    # Train model with cross validation\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean R-squared across folds: {results['cv_r2_mean']:.3f}\")\n",
    "    print(f\"Standard Deviation of R-squared: {results['cv_r2_std']:.3f}\")\n",
    "    print(f\"Mean RMSE across folds: ${results['cv_rmse_mean']:.2f}\")\n",
    "    print(f\"Standard Deviation of RMSE: ${results['cv_rmse_std']:.2f}\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    print(results['best_params'])\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(results['feature_importance'].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Random Forest with Additional Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of custom stopwords: 228\n",
      "Tuning general TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "General TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.451\n",
      "Standard Deviation: 0.011\n",
      "\n",
      "Tuning skill-specific TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Skill TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.280\n",
      "Standard Deviation: 0.019\n",
      "\n",
      "Processing full dataset with tuned parameters...\n",
      "\n",
      "Best General TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': False, 'tfidf__max_df': 0.7, 'tfidf__min_df': 0.05, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Best Skill TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': True, 'tfidf__max_df': 0.7, 'tfidf__min_df': 0.01, 'tfidf__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "Model Performance:\n",
      "Mean R-squared across folds: 0.618\n",
      "Standard Deviation of R-squared: 0.007\n",
      "Mean RMSE across folds: $26271.84\n",
      "Standard Deviation of RMSE: $196.27\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                       feature  importance\n",
      "1324  experience_level_encoded    0.029751\n",
      "1328              exp_industry    0.020966\n",
      "493        general_high school    0.020404\n",
      "283            general_diploma    0.015560\n",
      "349           general_engineer    0.014225\n",
      "350        general_engineering    0.013393\n",
      "938             general_school    0.013282\n",
      "590               general_lift    0.011856\n",
      "953             general_senior    0.010673\n",
      "939     general_school diploma    0.010384\n",
      "272             general_design    0.009369\n",
      "1010         general_strategic    0.009158\n",
      "502               general_hour    0.008764\n",
      "313               general_duty    0.008368\n",
      "825            general_project    0.007992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.best_general_tfidf = None\n",
    "        self.best_skill_tfidf = None\n",
    "        self.best_rf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Define high-frequency but low-predictive words\n",
    "        additional_stopwords = [\n",
    "            'work', 'experience', 'team', 'include', 'service', 'customer',\n",
    "            'provide', 'skill', 'year', 'job', 'support', 'opportunity',\n",
    "            'position', 'business', 'employee', 'company', 'benefit',\n",
    "            'require', 'ability', 'candidate', 'requirement', 'qualified',\n",
    "            'responsibilities', 'duties', 'role',\n",
    "            'years', 'including', 'must', 'will', 'new', 'looking', \n",
    "            'seeking', 'ideal', 'competitive', 'excellent', 'strong', \n",
    "            'great', 'minimum', 'preferred', 'qualification', 'knowledge', \n",
    "            'related', 'professional', 'proficiency', 'based', 'remote', \n",
    "            'hybrid', 'office', 'requirements', 'skills'\n",
    "        ]\n",
    "        \n",
    "        # Combine with standard stopwords\n",
    "        self.custom_stopwords = list(set(stopwords.words('english')).union(additional_stopwords))\n",
    "\n",
    "    def create_dual_tfidf_features(self, df, skill_categories):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features from both general text and skill-specific vocabulary with separate grid searches\n",
    "        and cross validation using R-squared as the scoring metric\n",
    "        \"\"\"\n",
    "        # Sample subset for initial tuning\n",
    "        sample_size = int(len(df) * 0.2)\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Combine text features\n",
    "        combined_text = df_sample['cleaned_title'] + ' ' + df_sample['cleaned_description']\n",
    "        \n",
    "        # 1. General TF-IDF Pipeline with custom stopwords\n",
    "        general_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=self.custom_stopwords)),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # General TF-IDF parameter grid\n",
    "        general_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.05],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # 2. Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        # Remove any stopwords from skill vocabulary\n",
    "        skill_vocabulary = skill_vocabulary - set(self.custom_stopwords)\n",
    "        \n",
    "        # Skill-specific TF-IDF Pipeline with custom stopwords\n",
    "        skill_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                vocabulary=list(skill_vocabulary),\n",
    "                stop_words=self.custom_stopwords\n",
    "            )),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # Skill-specific parameter grid\n",
    "        skill_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.05],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # Tune general TF-IDF with cross validation\n",
    "        print(\"Tuning general TF-IDF parameters...\")\n",
    "        general_grid = GridSearchCV(\n",
    "            general_pipeline,\n",
    "            general_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        general_grid.fit(combined_text, df_sample['salary'])\n",
    "        \n",
    "        # Print cross validation results for general TF-IDF\n",
    "        print(\"\\nGeneral TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(general_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {general_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[general_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Tune skill-specific TF-IDF with cross validation\n",
    "        print(\"\\nTuning skill-specific TF-IDF parameters...\")\n",
    "        skill_grid = GridSearchCV(\n",
    "            skill_pipeline,\n",
    "            skill_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        skill_grid.fit(combined_text, df_sample['salary'])\n",
    "        \n",
    "        # Print cross validation results for skill TF-IDF\n",
    "        print(\"\\nSkill TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(skill_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {skill_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[skill_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Process full dataset\n",
    "        print(\"\\nProcessing full dataset with tuned parameters...\")\n",
    "        combined_text_full = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Generate general TF-IDF features\n",
    "        self.best_general_tfidf = general_grid.best_estimator_.named_steps['tfidf']\n",
    "        general_matrix = self.best_general_tfidf.fit_transform(combined_text_full)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.best_general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Generate skill-specific TF-IDF features\n",
    "        self.best_skill_tfidf = skill_grid.best_estimator_.named_steps['tfidf']\n",
    "        skill_matrix = self.best_skill_tfidf.fit_transform(combined_text_full)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.best_skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Print best parameters for both grids\n",
    "        print(\"\\nBest General TF-IDF Parameters:\")\n",
    "        print(general_grid.best_params_)\n",
    "        print(\"\\nBest Skill TF-IDF Parameters:\")\n",
    "        print(skill_grid.best_params_)\n",
    "        \n",
    "        # Combine features\n",
    "        feature_df = pd.concat([general_features, skill_features], axis=1)\n",
    "        \n",
    "        # Add category-level aggregated features\n",
    "        for category, info in skill_categories.items():\n",
    "            keyword_cols = [col for col in skill_features.columns \n",
    "                          if any(keyword in col for keyword in info['keywords'])]\n",
    "            context_cols = [col for col in skill_features.columns \n",
    "                          if any(context in col for context in info['context_required'])]\n",
    "            \n",
    "            if keyword_cols:\n",
    "                feature_df[f'{category}_keyword_score'] = skill_features[keyword_cols].sum(axis=1) * 2\n",
    "            if context_cols:\n",
    "                feature_df[f'{category}_context_score'] = skill_features[context_cols].sum(axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['experience_level_encoded'] = self.label_encoder.fit_transform(df['experience_level'])\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'])\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'])\n",
    "        \n",
    "        # Add location features\n",
    "        feature_df['state'] = df['location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "        feature_df['state_encoded'] = self.label_encoder.fit_transform(feature_df['state'])\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_df['exp_industry'] = feature_df['experience_level_encoded'] * feature_df['industry_encoded']\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def train_model(self, feature_matrix, target):\n",
    "        \"\"\"\n",
    "        Train the final model using cross validation with R-squared as the main metric\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        X = feature_matrix.drop(['state'], axis=1)\n",
    "        y = target\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        # Define parameter grid for final model\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "\n",
    "        # Perform grid search with cross validation\n",
    "        rf_grid = GridSearchCV(\n",
    "            RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        \n",
    "        rf_grid.fit(X_scaled, y)\n",
    "        \n",
    "        # Store best model\n",
    "        self.best_rf = rf_grid.best_estimator_\n",
    "\n",
    "        # Perform cross validation on best model\n",
    "        r2_scores = cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Also calculate RMSE for reference\n",
    "        rmse_scores = np.sqrt(-cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Final evaluation metrics\n",
    "        final_results = {\n",
    "            'cv_r2_mean': r2_scores.mean(),\n",
    "            'cv_r2_std': r2_scores.std(),\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std(),\n",
    "            'best_params': rf_grid.best_params_,\n",
    "            'feature_importance': feature_importance,\n",
    "            'cv_r2_scores': r2_scores,\n",
    "            'cv_rmse_scores': rmse_scores\n",
    "        }\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Save all model components\n",
    "        \"\"\"\n",
    "        joblib.dump(self.best_general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.best_skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.best_rf, f'{path_prefix}random_forest.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "        joblib.dump(self.custom_stopwords, f'{path_prefix}custom_stopwords.joblib')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Load all model components\n",
    "        \"\"\"\n",
    "        self.best_general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.best_skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.best_rf = joblib.load(f'{path_prefix}random_forest.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "        self.custom_stopwords = joblib.load(f'{path_prefix}custom_stopwords.joblib')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Print the number of stopwords being used\n",
    "    print(f\"Number of custom stopwords: {len(model.custom_stopwords)}\")\n",
    "    \n",
    "    # Create features\n",
    "    feature_matrix = model.create_dual_tfidf_features(df, skill_categories)\n",
    "    \n",
    "    # Train model with cross validation\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean R-squared across folds: {results['cv_r2_mean']:.3f}\")\n",
    "    print(f\"Standard Deviation of R-squared: {results['cv_r2_std']:.3f}\")\n",
    "    print(f\"Mean RMSE across folds: ${results['cv_rmse_mean']:.2f}\")\n",
    "    print(f\"Standard Deviation of RMSE: ${results['cv_rmse_std']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(results['feature_importance'].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Random Forest with BERT embedding in Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for job titles...\n",
      "Generating TF-IDF features...\n",
      "Tuning general TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.488\n",
      "Standard Deviation: 0.002\n",
      "\n",
      "Tuning skill-specific TF-IDF parameters...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skill TF-IDF Cross Validation Results:\n",
      "Best CV R-squared Score: 0.296\n",
      "Standard Deviation: 0.005\n",
      "\n",
      "Processing full dataset with tuned parameters...\n",
      "\n",
      "Best General TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': False, 'tfidf__max_df': 0.9, 'tfidf__min_df': 0.05, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Best Skill TF-IDF Parameters:\n",
      "{'rf__max_depth': 10, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100, 'tfidf__binary': True, 'tfidf__max_df': 0.7, 'tfidf__min_df': 0.01, 'tfidf__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "Model Performance:\n",
      "Mean R-squared across folds: 0.619\n",
      "Standard Deviation of R-squared: 0.007\n",
      "Mean RMSE across folds: $26256.66\n",
      "Standard Deviation of RMSE: $207.74\n",
      "\n",
      "BERT Features Total Importance: 0.528\n",
      "\n",
      "Best Parameters:\n",
      "{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                       feature  importance\n",
      "2135  experience_level_encoded    0.016734\n",
      "2139              exp_industry    0.014093\n",
      "493        general_high school    0.011684\n",
      "977     general_school diploma    0.011414\n",
      "1334          skill_experience    0.011093\n",
      "287            general_diploma    0.008907\n",
      "976             general_school    0.008320\n",
      "250   general_customer service    0.008240\n",
      "357        general_engineering    0.008235\n",
      "603               general_lift    0.008035\n",
      "990             general_senior    0.008013\n",
      "356           general_engineer    0.007896\n",
      "281            general_develop    0.007275\n",
      "1622              bert_dim_275    0.007098\n",
      "277             general_design    0.005859\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.best_general_tfidf = None\n",
    "        self.best_skill_tfidf = None\n",
    "        self.best_rf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Initialize BERT model and tokenizer\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_bert_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate BERT embeddings for a list of texts\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                # Tokenize and encode text\n",
    "                encoded = self.bert_tokenizer(\n",
    "                    text,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Get BERT output\n",
    "                outputs = self.bert_model(**encoded)\n",
    "                \n",
    "                # Use [CLS] token embedding (first token)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "                embeddings.append(embedding[0])\n",
    "                \n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def create_dual_tfidf_features(self, df, skill_categories):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features and BERT embeddings\n",
    "        \"\"\"\n",
    "        # Get BERT embeddings for job titles\n",
    "        print(\"Generating BERT embeddings for job titles...\")\n",
    "        bert_embeddings = self.get_bert_embeddings(df['cleaned_title'])\n",
    "        bert_features = pd.DataFrame(\n",
    "            bert_embeddings,\n",
    "            columns=[f'bert_dim_{i}' for i in range(bert_embeddings.shape[1])]\n",
    "        )\n",
    "        \n",
    "        # Generate TF-IDF features using full dataset\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # 1. General TF-IDF Pipeline\n",
    "        general_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(stop_words=\"english\")),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # General TF-IDF parameter grid\n",
    "        general_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.05],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # 2. Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        # Skill-specific TF-IDF Pipeline\n",
    "        skill_pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                vocabulary=list(skill_vocabulary),\n",
    "                stop_words=\"english\"\n",
    "            )),\n",
    "            ('rf', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # Skill-specific parameter grid\n",
    "        skill_param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [0.01, 0.02, 0.03],\n",
    "            'tfidf__max_df': [0.7, 0.8, 0.9],\n",
    "            'tfidf__binary': [False, True],\n",
    "            'rf__n_estimators': [100],\n",
    "            'rf__max_depth': [10],\n",
    "            'rf__min_samples_split': [5],\n",
    "            'rf__min_samples_leaf': [2],\n",
    "            'rf__max_features': ['sqrt']\n",
    "        }\n",
    "        \n",
    "        # Tune general TF-IDF with cross validation\n",
    "        print(\"Tuning general TF-IDF parameters...\")\n",
    "        general_grid = GridSearchCV(\n",
    "            general_pipeline,\n",
    "            general_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        general_grid.fit(combined_text, df['salary'])\n",
    "        \n",
    "        # Print cross validation results for general TF-IDF\n",
    "        print(\"\\nGeneral TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(general_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {general_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[general_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Tune skill-specific TF-IDF with cross validation\n",
    "        print(\"\\nTuning skill-specific TF-IDF parameters...\")\n",
    "        skill_grid = GridSearchCV(\n",
    "            skill_pipeline,\n",
    "            skill_param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        skill_grid.fit(combined_text, df['salary'])\n",
    "        \n",
    "        # Print cross validation results for skill TF-IDF\n",
    "        print(\"\\nSkill TF-IDF Cross Validation Results:\")\n",
    "        cv_results = pd.DataFrame(skill_grid.cv_results_)\n",
    "        print(f\"Best CV R-squared Score: {skill_grid.best_score_:.3f}\")\n",
    "        print(f\"Standard Deviation: {cv_results.loc[skill_grid.best_index_, 'std_test_score']:.3f}\")\n",
    "        \n",
    "        # Generate features for full dataset using best parameters\n",
    "        print(\"\\nProcessing full dataset with tuned parameters...\")\n",
    "        \n",
    "        # Generate general TF-IDF features\n",
    "        self.best_general_tfidf = general_grid.best_estimator_.named_steps['tfidf']\n",
    "        general_matrix = self.best_general_tfidf.fit_transform(combined_text)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.best_general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Generate skill-specific TF-IDF features\n",
    "        self.best_skill_tfidf = skill_grid.best_estimator_.named_steps['tfidf']\n",
    "        skill_matrix = self.best_skill_tfidf.fit_transform(combined_text)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.best_skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Print best parameters\n",
    "        print(\"\\nBest General TF-IDF Parameters:\")\n",
    "        print(general_grid.best_params_)\n",
    "        print(\"\\nBest Skill TF-IDF Parameters:\")\n",
    "        print(skill_grid.best_params_)\n",
    "        \n",
    "        # Combine all features\n",
    "        feature_df = pd.concat([\n",
    "            general_features,\n",
    "            skill_features,\n",
    "            bert_features\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Add category-level aggregated features\n",
    "        for category, info in skill_categories.items():\n",
    "            keyword_cols = [col for col in skill_features.columns \n",
    "                          if any(keyword in col for keyword in info['keywords'])]\n",
    "            context_cols = [col for col in skill_features.columns \n",
    "                          if any(context in col for context in info['context_required'])]\n",
    "            \n",
    "            if keyword_cols:\n",
    "                feature_df[f'{category}_keyword_score'] = skill_features[keyword_cols].sum(axis=1) * 2\n",
    "            if context_cols:\n",
    "                feature_df[f'{category}_context_score'] = skill_features[context_cols].sum(axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['experience_level_encoded'] = self.label_encoder.fit_transform(df['experience_level'])\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'])\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'])\n",
    "        \n",
    "        # Add location features\n",
    "        feature_df['state'] = df['location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "        feature_df['state_encoded'] = self.label_encoder.fit_transform(feature_df['state'])\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_df['exp_industry'] = feature_df['experience_level_encoded'] * feature_df['industry_encoded']\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def train_model(self, feature_matrix, target):\n",
    "        \"\"\"\n",
    "        Train model with enhanced feature set including BERT embeddings\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        X = feature_matrix.drop(['state'], axis=1)\n",
    "        y = target\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        # Enhanced parameter grid for final model\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "        }\n",
    "\n",
    "        # Grid search with cross validation\n",
    "        rf_grid = GridSearchCV(\n",
    "            RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            param_grid,\n",
    "            cv=self.cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='r2'\n",
    "        )\n",
    "        \n",
    "        rf_grid.fit(X_scaled, y)\n",
    "        self.best_rf = rf_grid.best_estimator_\n",
    "\n",
    "        # Cross validation metrics\n",
    "        r2_scores = cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rmse_scores = np.sqrt(-cross_val_score(\n",
    "            self.best_rf,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=self.cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "        \n",
    "        # Calculate feature importance with BERT features\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Aggregate importance of BERT features\n",
    "        bert_importance = feature_importance[\n",
    "            feature_importance['feature'].str.startswith('bert_')\n",
    "        ]['importance'].sum()\n",
    "\n",
    "        final_results = {\n",
    "            'cv_r2_mean': r2_scores.mean(),\n",
    "            'cv_r2_std': r2_scores.std(),\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std(),\n",
    "            'best_params': rf_grid.best_params_,\n",
    "            'feature_importance': feature_importance,\n",
    "            'bert_importance': bert_importance,\n",
    "            'cv_r2_scores': r2_scores,\n",
    "            'cv_rmse_scores': rmse_scores\n",
    "        }\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Save all model components including BERT\n",
    "        \"\"\"\n",
    "        joblib.dump(self.best_general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.best_skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.best_rf, f'{path_prefix}random_forest.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "        # BERT models are saved separately using their own save methods\n",
    "        self.bert_tokenizer.save_pretrained(f'{path_prefix}bert_tokenizer')\n",
    "        self.bert_model.save_pretrained(f'{path_prefix}bert_model')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"\n",
    "        Load all model components including BERT\n",
    "        \"\"\"\n",
    "        self.best_general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.best_skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.best_rf = joblib.load(f'{path_prefix}random_forest.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "        # Load BERT models\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(f'{path_prefix}bert_tokenizer')\n",
    "        self.bert_model = AutoModel.from_pretrained(f'{path_prefix}bert_model')\n",
    "        self.bert_model.eval()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Create features with BERT embeddings\n",
    "    feature_matrix = model.create_dual_tfidf_features(df, skill_categories)\n",
    "    \n",
    "    # Train model\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean R-squared across folds: {results['cv_r2_mean']:.3f}\")\n",
    "    print(f\"Standard Deviation of R-squared: {results['cv_r2_std']:.3f}\")\n",
    "    print(f\"Mean RMSE across folds: ${results['cv_rmse_mean']:.2f}\")\n",
    "    print(f\"Standard Deviation of RMSE: ${results['cv_rmse_std']:.2f}\")\n",
    "    print(f\"\\nBERT Features Total Importance: {results['bert_importance']:.3f}\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    print(results['best_params'])\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(results['feature_importance'].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF * Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Generating TF-IDF features...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:03<00:00, 355.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Average training loss: 0.5830\n",
      "Average validation loss: 0.3736\n",
      "RMSE: $26,035.52\n",
      "MAE: $19,675.51\n",
      "R2 Score: 0.6301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:03<00:00, 329.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Average training loss: 0.3084\n",
      "Average validation loss: 0.3255\n",
      "RMSE: $24,301.98\n",
      "MAE: $18,126.24\n",
      "R2 Score: 0.6777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:03<00:00, 397.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Average training loss: 0.2388\n",
      "Average validation loss: 0.3093\n",
      "RMSE: $23,688.14\n",
      "MAE: $17,275.88\n",
      "R2 Score: 0.6938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:02<00:00, 415.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Average training loss: 0.1930\n",
      "Average validation loss: 0.3029\n",
      "RMSE: $23,440.87\n",
      "MAE: $16,985.59\n",
      "R2 Score: 0.7002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:02<00:00, 420.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Average training loss: 0.1572\n",
      "Average validation loss: 0.3000\n",
      "RMSE: $23,328.39\n",
      "MAE: $16,706.25\n",
      "R2 Score: 0.7030\n",
      "\n",
      "\n",
      "Final Model Performance:\n",
      "RMSE: $23,328.39\n",
      "MAE: $16,706.25\n",
      "R2 Score: 0.7030\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SalaryDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets) if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.features[idx], self.targets[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "class SalaryNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SalaryNeuralNet, self).__init__()\n",
    "        \n",
    "        # Process TF-IDF features\n",
    "        self.tfidf_fc = nn.Linear(input_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Regressor matches the structure from the reference model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process TF-IDF features\n",
    "        x = self.tfidf_fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Final regression\n",
    "        return self.regressor(x).squeeze()\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.general_tfidf = None\n",
    "        self.skill_tfidf = None\n",
    "        self.neural_net = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.salary_scaler = StandardScaler()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def create_features(self, df, skill_categories):\n",
    "        \"\"\"Create TF-IDF and other features\"\"\"\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # General TF-IDF\n",
    "        self.general_tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=0.02,\n",
    "            max_df=0.8\n",
    "        )\n",
    "        general_matrix = self.general_tfidf.fit_transform(combined_text)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Skill-specific TF-IDF\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        self.skill_tfidf = TfidfVectorizer(\n",
    "            vocabulary=list(skill_vocabulary),\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        skill_matrix = self.skill_tfidf.fit_transform(combined_text)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Combine all features\n",
    "        feature_df = pd.concat([\n",
    "            general_features,\n",
    "            skill_features\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'].fillna('UNKNOWN'))\n",
    "        feature_df['company_encoded'] = self.label_encoder.fit_transform(df['company'].fillna('UNKNOWN'))\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'].fillna('UNKNOWN'))\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def prepare_data(self, feature_matrix, target, batch_size=16):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(feature_matrix)\n",
    "        y_scaled = self.salary_scaler.fit_transform(target.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = SalaryDataset(X_scaled, y_scaled)\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, feature_matrix, target, epochs=5, batch_size=16, learning_rate=2e-5):\n",
    "        \"\"\"Train neural network model\"\"\"\n",
    "        # Prepare data\n",
    "        train_loader, val_loader = self.prepare_data(feature_matrix, target, batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.neural_net = SalaryNeuralNet(input_dim=feature_matrix.shape[1]).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.neural_net.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        training_history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.neural_net.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for features, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "                features = features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.neural_net(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            training_history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.neural_net.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_preds = []\n",
    "            val_true = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.neural_net(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    val_preds.extend(outputs.cpu().numpy())\n",
    "                    val_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            training_history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Transform predictions back to original scale\n",
    "            val_preds = self.salary_scaler.inverse_transform(np.array(val_preds).reshape(-1, 1))\n",
    "            val_true = self.salary_scaler.inverse_transform(np.array(val_true).reshape(-1, 1))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
    "            mae = mean_absolute_error(val_true, val_preds)\n",
    "            r2 = r2_score(val_true, val_preds)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}:')\n",
    "            print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "            print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "            print(f'RMSE: ${rmse:,.2f}')\n",
    "            print(f'MAE: ${mae:,.2f}')\n",
    "            print(f'R2 Score: {r2:.4f}\\n')\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(self.neural_net.state_dict(), 'best_salary_predictor.pth')\n",
    "        \n",
    "        return {\n",
    "            'training_history': training_history,\n",
    "            'final_metrics': {\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"Save all model components\"\"\"\n",
    "        joblib.dump(self.general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "        joblib.dump(self.salary_scaler, f'{path_prefix}salary_scaler.joblib')\n",
    "        torch.save(self.neural_net.state_dict(), f'{path_prefix}neural_net.pt')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"Load all model components\"\"\"\n",
    "        self.general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "        self.salary_scaler = joblib.load(f'{path_prefix}salary_scaler.joblib')\n",
    "        self.neural_net.load_state_dict(torch.load(f'{path_prefix}neural_net.pt'))\n",
    "        self.neural_net.eval()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = df\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Create features\n",
    "    feature_matrix = model.create_features(df, skill_categories)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    results = model.train_model(feature_matrix, df['salary'])\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    print(f\"RMSE: ${results['final_metrics']['RMSE']:,.2f}\")\n",
    "    print(f\"MAE: ${results['final_metrics']['MAE']:,.2f}\")\n",
    "    print(f\"R2 Score: {results['final_metrics']['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF General Hyper-Parameter Tunning * Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Tuning TF-IDF parameters...\n",
      "Tuning TF-IDF parameters...\n",
      "\n",
      "Tuning general TF-IDF parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:   1%|          | 1/81 [00:03<04:01,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 1), 'min_df': 0.01, 'max_df': 0.7, 'max_features': None}\n",
      "Features: 2598\n",
      "Sparsity: 0.9250\n",
      "Score: 194.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:   5%|▍         | 4/81 [00:11<03:45,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 1), 'min_df': 0.01, 'max_df': 0.8, 'max_features': None}\n",
      "Features: 2602\n",
      "Sparsity: 0.9239\n",
      "Score: 197.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:   9%|▊         | 7/81 [00:20<03:36,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 1), 'min_df': 0.01, 'max_df': 0.9, 'max_features': None}\n",
      "Features: 2604\n",
      "Sparsity: 0.9234\n",
      "Score: 199.5817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  35%|███▍      | 28/81 [01:30<04:08,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 2), 'min_df': 0.01, 'max_df': 0.7, 'max_features': None}\n",
      "Features: 4955\n",
      "Sparsity: 0.9483\n",
      "Score: 256.2993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  38%|███▊      | 31/81 [01:58<06:13,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 2), 'min_df': 0.01, 'max_df': 0.8, 'max_features': None}\n",
      "Features: 4959\n",
      "Sparsity: 0.9477\n",
      "Score: 259.3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  42%|████▏     | 34/81 [02:27<06:53,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 2), 'min_df': 0.01, 'max_df': 0.9, 'max_features': None}\n",
      "Features: 4961\n",
      "Sparsity: 0.9474\n",
      "Score: 260.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  68%|██████▊   | 55/81 [05:39<04:32, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 3), 'min_df': 0.01, 'max_df': 0.7, 'max_features': None}\n",
      "Features: 5536\n",
      "Sparsity: 0.9510\n",
      "Score: 271.3669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  72%|███████▏  | 58/81 [06:44<06:32, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 3), 'min_df': 0.01, 'max_df': 0.8, 'max_features': None}\n",
      "Features: 5540\n",
      "Sparsity: 0.9505\n",
      "Score: 274.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters:  75%|███████▌  | 61/81 [07:47<06:17, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best general parameters found:\n",
      "Parameters: {'ngram_range': (1, 3), 'min_df': 0.01, 'max_df': 0.9, 'max_features': None}\n",
      "Features: 5542\n",
      "Sparsity: 0.9502\n",
      "Score: 275.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing general TF-IDF parameters: 100%|██████████| 81/81 [14:48<00:00, 10.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning skill-specific TF-IDF parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing skill TF-IDF parameters: 100%|██████████| 8/8 [00:00<00:00, 12048.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with parameters {'ngram_range': (1, 1), 'min_df': 0.01, 'max_df': 0.8}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 1), 'min_df': 0.01, 'max_df': 0.9}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 1), 'min_df': 0.02, 'max_df': 0.8}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 1), 'min_df': 0.02, 'max_df': 0.9}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 2), 'min_df': 0.01, 'max_df': 0.8}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 2), 'min_df': 0.01, 'max_df': 0.9}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 2), 'min_df': 0.02, 'max_df': 0.8}: empty vocabulary passed to fit\n",
      "Error with parameters {'ngram_range': (1, 2), 'min_df': 0.02, 'max_df': 0.9}: empty vocabulary passed to fit\n",
      "\n",
      "Best general TF-IDF parameters: {'ngram_range': (1, 3), 'min_df': 0.01, 'max_df': 0.9, 'max_features': None}\n",
      "Best skill TF-IDF parameters: None\n",
      "\n",
      "Creating features...\n",
      "Generating TF-IDF features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary passed to fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 518\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Create features using tuned TF-IDF parameters\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 518\u001b[0m feature_matrix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcreate_features(df, skill_categories)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Tune neural network hyperparameters\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTuning neural network hyperparameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 306\u001b[0m, in \u001b[0;36mSalaryPredictionModel.create_features\u001b[0;34m(self, df, skill_categories)\u001b[0m\n\u001b[1;32m    299\u001b[0m     skill_vocabulary\u001b[38;5;241m.\u001b[39mupdate(category[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_required\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m    302\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(skill_vocabulary),\n\u001b[1;32m    303\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mskill_params\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 306\u001b[0m skill_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_tfidf\u001b[38;5;241m.\u001b[39mfit_transform(combined_text)\n\u001b[1;32m    307\u001b[0m skill_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    308\u001b[0m     skill_matrix\u001b[38;5;241m.\u001b[39mtoarray(),\n\u001b[1;32m    309\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskill_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_tfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out()]\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1356\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m-> 1356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[1;32m   1357\u001b[0m max_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_df\n\u001b[1;32m   1358\u001b[0m min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:494\u001b[0m, in \u001b[0;36m_VectorizerMixin._validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary passed to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary passed to fit"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from itertools import product\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SalaryDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets) if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.features[idx], self.targets[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "class SalaryNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout_rate=0.1):\n",
    "        super(SalaryNeuralNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.general_tfidf = None\n",
    "        self.skill_tfidf = None\n",
    "        self.neural_net = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.salary_scaler = StandardScaler()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Best parameters storage\n",
    "        self.best_general_tfidf_params = None\n",
    "        self.best_skill_tfidf_params = None\n",
    "        self.best_nn_params = None\n",
    "\n",
    "    def prepare_data(self, feature_matrix, target, batch_size=16):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(feature_matrix)\n",
    "        y_scaled = self.salary_scaler.fit_transform(target.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = SalaryDataset(X_scaled, y_scaled)\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def tune_tfidf_parameters(self, df, skill_categories):\n",
    "        \"\"\"Tune TF-IDF parameters separately for general and skill-specific vectorizers\"\"\"\n",
    "        print(\"Tuning TF-IDF parameters...\")\n",
    "        \n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # TF-IDF parameter grid\n",
    "        general_param_grid = {\n",
    "            'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "            'min_df': [0.01, 0.02, 0.05],\n",
    "            'max_df': [0.7, 0.8, 0.9],\n",
    "            'max_features': [None, 1000, 5000]\n",
    "        }\n",
    "        \n",
    "        skill_param_grid = {\n",
    "            'ngram_range': [(1, 1), (1, 2)],\n",
    "            'min_df': [0.01, 0.02],\n",
    "            'max_df': [0.8, 0.9]\n",
    "        }\n",
    "        \n",
    "        # Tune general TF-IDF\n",
    "        print(\"\\nTuning general TF-IDF parameters...\")\n",
    "        best_general_score = float('-inf')\n",
    "        best_general_params = None\n",
    "        \n",
    "        param_combinations = [dict(zip(general_param_grid.keys(), v)) \n",
    "                            for v in product(*general_param_grid.values())]\n",
    "        \n",
    "        for params in tqdm(param_combinations, desc=\"Testing general TF-IDF parameters\"):\n",
    "            try:\n",
    "                general_tfidf = TfidfVectorizer(\n",
    "                    stop_words=\"english\",\n",
    "                    **params\n",
    "                )\n",
    "                \n",
    "                feature_matrix = general_tfidf.fit_transform(combined_text)\n",
    "                n_features = feature_matrix.shape[1]\n",
    "                sparsity = 1.0 - (feature_matrix.nnz / (feature_matrix.shape[0] * feature_matrix.shape[1]))\n",
    "                score = n_features * (1 - sparsity)\n",
    "                \n",
    "                if score > best_general_score:\n",
    "                    best_general_score = score\n",
    "                    best_general_params = params\n",
    "                    print(f\"\\nNew best general parameters found:\")\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Features: {n_features}\")\n",
    "                    print(f\"Sparsity: {sparsity:.4f}\")\n",
    "                    print(f\"Score: {score:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Tune skill-specific TF-IDF\n",
    "        print(\"\\nTuning skill-specific TF-IDF parameters...\")\n",
    "        best_skill_score = float('-inf')\n",
    "        best_skill_params = None\n",
    "        \n",
    "        # Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        param_combinations = [dict(zip(skill_param_grid.keys(), v)) \n",
    "                            for v in product(*skill_param_grid.values())]\n",
    "        \n",
    "        for params in tqdm(param_combinations, desc=\"Testing skill TF-IDF parameters\"):\n",
    "            try:\n",
    "                skill_tfidf = TfidfVectorizer(\n",
    "                    vocabulary=list(skill_vocabulary),\n",
    "                    stop_words=\"english\",\n",
    "                    **params\n",
    "                )\n",
    "                \n",
    "                feature_matrix = skill_tfidf.fit_transform(combined_text)\n",
    "                n_features = feature_matrix.shape[1]\n",
    "                sparsity = 1.0 - (feature_matrix.nnz / (feature_matrix.shape[0] * feature_matrix.shape[1]))\n",
    "                score = n_features * (1 - sparsity)\n",
    "                \n",
    "                if score > best_skill_score:\n",
    "                    best_skill_score = score\n",
    "                    best_skill_params = params\n",
    "                    print(f\"\\nNew best skill parameters found:\")\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Features: {n_features}\")\n",
    "                    print(f\"Sparsity: {sparsity:.4f}\")\n",
    "                    print(f\"Score: {score:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_general_tfidf_params = best_general_params\n",
    "        self.best_skill_tfidf_params = best_skill_params\n",
    "        \n",
    "        return {\n",
    "            'general': best_general_params,\n",
    "            'skill': best_skill_params\n",
    "        }\n",
    "\n",
    "    def tune_neural_network(self, feature_matrix, target, max_trials=10):\n",
    "        \"\"\"Tune neural network hyperparameters\"\"\"\n",
    "        print(\"Tuning neural network hyperparameters...\")\n",
    "        \n",
    "        # Neural network parameter grid\n",
    "        nn_param_grid = {\n",
    "            'hidden_dims': [\n",
    "                [256, 128, 64],\n",
    "                [512, 256, 128],\n",
    "                [256, 256, 128],\n",
    "                [512, 256, 128, 64]\n",
    "            ],\n",
    "            'dropout_rate': [0.1, 0.2, 0.3],\n",
    "            'learning_rate': [1e-4, 2e-4, 5e-4],\n",
    "            'batch_size': [16, 32, 64]\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        param_combinations = [dict(zip(nn_param_grid.keys(), v)) \n",
    "                            for v in product(*nn_param_grid.values())]\n",
    "        \n",
    "        # Randomly sample from parameter combinations if there are too many\n",
    "        if len(param_combinations) > max_trials:\n",
    "            param_combinations = np.random.choice(\n",
    "                param_combinations, \n",
    "                size=max_trials, \n",
    "                replace=False\n",
    "            )\n",
    "        \n",
    "        for params in tqdm(param_combinations, desc=\"Testing NN parameters\"):\n",
    "            try:\n",
    "                # Prepare data with current batch size\n",
    "                train_loader, val_loader = self.prepare_data(\n",
    "                    feature_matrix, \n",
    "                    target, \n",
    "                    batch_size=params['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Initialize model with current parameters\n",
    "                self.neural_net = SalaryNeuralNet(\n",
    "                    input_dim=feature_matrix.shape[1],\n",
    "                    hidden_dims=params['hidden_dims'],\n",
    "                    dropout_rate=params['dropout_rate']\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Train for a few epochs to evaluate parameters\n",
    "                results = self.train_model(\n",
    "                    feature_matrix,\n",
    "                    target,\n",
    "                    epochs=5,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    is_tuning=True\n",
    "                )\n",
    "                \n",
    "                val_loss = min(results['training_history']['val_loss'])\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = params\n",
    "                    print(f\"\\nNew best parameters found:\")\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_nn_params = best_params\n",
    "        return best_params\n",
    "\n",
    "    def create_features(self, df, skill_categories):\n",
    "        \"\"\"Create TF-IDF and other features using best parameters\"\"\"\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # Use tuned parameters if available for general TF-IDF\n",
    "        general_params = self.best_general_tfidf_params or {\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 0.02,\n",
    "            'max_df': 0.8\n",
    "        }\n",
    "        \n",
    "        # General TF-IDF\n",
    "        self.general_tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            **general_params\n",
    "        )\n",
    "        general_matrix = self.general_tfidf.fit_transform(combined_text)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Use tuned parameters if available for skill TF-IDF\n",
    "        skill_params = self.best_skill_tfidf_params or {\n",
    "            'ngram_range': (1, 2),\n",
    "            'min_df': 0.02,\n",
    "            'max_df': 0.8\n",
    "        }\n",
    "        \n",
    "        # Skill-specific TF-IDF\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        self.skill_tfidf = TfidfVectorizer(\n",
    "            vocabulary=list(skill_vocabulary),\n",
    "            stop_words=\"english\",\n",
    "            **skill_params\n",
    "        )\n",
    "        skill_matrix = self.skill_tfidf.fit_transform(combined_text)\n",
    "        skill_features = pd.DataFrame(\n",
    "            skill_matrix.toarray(),\n",
    "            columns=[f'skill_{f}' for f in self.skill_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        feature_df = pd.concat([\n",
    "            general_features,\n",
    "            skill_features\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'].fillna('UNKNOWN'))\n",
    "        feature_df['company_encoded'] = self.label_encoder.fit_transform(df['company'].fillna('UNKNOWN'))\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'].fillna('UNKNOWN'))\n",
    "        \n",
    "        return feature_df\n",
    "\n",
    "    def train_model(self, feature_matrix, target, epochs=10, batch_size=None, \n",
    "                   learning_rate=None, is_tuning=False):\n",
    "        \"\"\"Train neural network model using best parameters\"\"\"\n",
    "        # Use default parameters if none provided and not in tuning mode\n",
    "        if not is_tuning:\n",
    "            if self.best_nn_params:\n",
    "                batch_size = batch_size or self.best_nn_params.get('batch_size', 16)\n",
    "                learning_rate = learning_rate or self.best_nn_params.get('learning_rate', 2e-5)\n",
    "            else:\n",
    "                batch_size = batch_size or 16\n",
    "                learning_rate = learning_rate or 2e-5\n",
    "        \n",
    "        # Prepare data\n",
    "        train_loader, val_loader = self.prepare_data(feature_matrix, target, batch_size)\n",
    "        \n",
    "        # Initialize model if not already initialized\n",
    "        if not self.neural_net or is_tuning:\n",
    "            hidden_dims = self.best_nn_params.get('hidden_dims', [256, 128, 64]) if self.best_nn_params else [256, 128, 64]\n",
    "            dropout_rate = self.best_nn_params.get('dropout_rate', 0.1) if self.best_nn_params else 0.1\n",
    "            \n",
    "            self.neural_net = SalaryNeuralNet(\n",
    "                input_dim=feature_matrix.shape[1],\n",
    "                hidden_dims=hidden_dims,\n",
    "                dropout_rate=dropout_rate\n",
    "            ).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.neural_net.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        training_history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.neural_net.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for features, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "                features = features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.neural_net(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            training_history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.neural_net.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_preds = []\n",
    "            val_true = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.neural_net(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    val_preds.extend(outputs.cpu().numpy())\n",
    "                    val_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            training_history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            if not is_tuning:\n",
    "                # Transform predictions back to original scale\n",
    "                val_preds = self.salary_scaler.inverse_transform(np.array(val_preds).reshape(-1, 1))\n",
    "                val_true = self.salary_scaler.inverse_transform(np.array(val_true).reshape(-1, 1))\n",
    "                \n",
    "                # Calculate metrics\n",
    "                rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
    "                mae = mean_absolute_error(val_true, val_preds)\n",
    "                r2 = r2_score(val_true, val_preds)\n",
    "                \n",
    "                print(f'Epoch {epoch + 1}:')\n",
    "                print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "                print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "                print(f'RMSE: ${rmse:,.2f}')\n",
    "                print(f'MAE: ${mae:,.2f}')\n",
    "                print(f'R2 Score: {r2:.4f}\\n')\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                if not is_tuning:\n",
    "                    torch.save(self.neural_net.state_dict(), 'best_salary_predictor.pth')\n",
    "        \n",
    "        if not is_tuning:\n",
    "            final_metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            }\n",
    "        else:\n",
    "            final_metrics = {}\n",
    "        \n",
    "        return {\n",
    "            'training_history': training_history,\n",
    "            'final_metrics': final_metrics\n",
    "        }\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"Save all model components\"\"\"\n",
    "        import os\n",
    "        os.makedirs(path_prefix, exist_ok=True)\n",
    "        \n",
    "        joblib.dump(self.general_tfidf, f'{path_prefix}general_tfidf.joblib')\n",
    "        joblib.dump(self.skill_tfidf, f'{path_prefix}skill_tfidf.joblib')\n",
    "        joblib.dump(self.scaler, f'{path_prefix}scaler.joblib')\n",
    "        joblib.dump(self.label_encoder, f'{path_prefix}label_encoder.joblib')\n",
    "        joblib.dump(self.salary_scaler, f'{path_prefix}salary_scaler.joblib')\n",
    "        joblib.dump(self.best_general_tfidf_params, f'{path_prefix}best_general_tfidf_params.joblib')\n",
    "        joblib.dump(self.best_skill_tfidf_params, f'{path_prefix}best_skill_tfidf_params.joblib')\n",
    "        joblib.dump(self.best_nn_params, f'{path_prefix}best_nn_params.joblib')\n",
    "        torch.save(self.neural_net.state_dict(), f'{path_prefix}neural_net.pt')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"Load all model components\"\"\"\n",
    "        self.general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "        self.salary_scaler = joblib.load(f'{path_prefix}salary_scaler.joblib')\n",
    "        self.best_general_tfidf_params = joblib.load(f'{path_prefix}best_general_tfidf_params.joblib')\n",
    "        self.best_skill_tfidf_params = joblib.load(f'{path_prefix}best_skill_tfidf_params.joblib')\n",
    "        self.best_nn_params = joblib.load(f'{path_prefix}best_nn_params.joblib')\n",
    "        \n",
    "        # Initialize neural network with best parameters\n",
    "        self.neural_net = SalaryNeuralNet(\n",
    "            input_dim=self.scaler.n_features_in_,\n",
    "            hidden_dims=self.best_nn_params['hidden_dims'],\n",
    "            dropout_rate=self.best_nn_params['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        self.neural_net.load_state_dict(torch.load(f'{path_prefix}neural_net.pt'))\n",
    "        self.neural_net.eval()\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        # Create features\n",
    "        features = self.create_features(df)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.neural_net(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # Scale back predictions\n",
    "        predictions = self.salary_scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "        \n",
    "        return predictions.flatten()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = df  # Your DataFrame\n",
    "    skill_categories = {\n",
    "        # Your skill categories dictionary\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Tune TF-IDF parameters\n",
    "    print(\"Tuning TF-IDF parameters...\")\n",
    "    tfidf_params = model.tune_tfidf_parameters(df, skill_categories)\n",
    "    print(\"\\nBest general TF-IDF parameters:\", tfidf_params['general'])\n",
    "    print(\"Best skill TF-IDF parameters:\", tfidf_params['skill'])\n",
    "    \n",
    "    # Create features using tuned TF-IDF parameters\n",
    "    print(\"\\nCreating features...\")\n",
    "    feature_matrix = model.create_features(df, skill_categories)\n",
    "    \n",
    "    # Tune neural network hyperparameters\n",
    "    print(\"\\nTuning neural network hyperparameters...\")\n",
    "    nn_params = model.tune_neural_network(feature_matrix, df['salary'])\n",
    "    print(\"\\nBest neural network parameters:\", nn_params)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    results = model.train_model(\n",
    "        feature_matrix,\n",
    "        df['salary'],\n",
    "        epochs=20  # Increase epochs for final training\n",
    "    )\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    print(f\"RMSE: ${results['final_metrics']['RMSE']:,.2f}\")\n",
    "    print(f\"MAE: ${results['final_metrics']['MAE']:,.2f}\")\n",
    "    print(f\"R2 Score: {results['final_metrics']['R2']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving model...\")\n",
    "    model.save_model()\n",
    "    \n",
    "    # Plot training history\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['training_history']['train_loss'], label='Training Loss')\n",
    "    plt.plot(results['training_history']['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF skills Hyper-Parameter Tunning * Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Tuning skill-specific TF-IDF parameters...\n",
      "Tuning skill-specific TF-IDF parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing skill TF-IDF parameters:   4%|▎         | 1/27 [00:03<01:28,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best skill parameters found:\n",
      "Parameters: {'ngram_range': (1, 1), 'min_df': 0.005, 'max_df': 0.8}\n",
      "Features: 157\n",
      "Sparsity: 0.9629\n",
      "Average Frequency: 298.0522\n",
      "Score: 1735.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing skill TF-IDF parameters:  37%|███▋      | 10/27 [00:36<01:12,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best skill parameters found:\n",
      "Parameters: {'ngram_range': (1, 2), 'min_df': 0.005, 'max_df': 0.8}\n",
      "Features: 157\n",
      "Sparsity: 0.9589\n",
      "Average Frequency: 311.7437\n",
      "Score: 2009.7927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing skill TF-IDF parameters:  70%|███████   | 19/27 [01:37<00:58,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best skill parameters found:\n",
      "Parameters: {'ngram_range': (1, 3), 'min_df': 0.005, 'max_df': 0.8}\n",
      "Features: 157\n",
      "Sparsity: 0.9589\n",
      "Average Frequency: 311.8313\n",
      "Score: 2011.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing skill TF-IDF parameters: 100%|██████████| 27/27 [02:54<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Skill TF-IDF Parameters:\n",
      "N-gram Range: (1, 3)\n",
      "Min Document Frequency: 0.005\n",
      "Max Document Frequency: 0.8\n",
      "\n",
      "Feature Statistics:\n",
      "Number of Features: 157\n",
      "Sparsity: 0.9589\n",
      "Average Feature Frequency: 311.8313\n",
      "Maximum Feature Frequency: 8695.6333\n",
      "\n",
      "Creating features...\n",
      "Generating TF-IDF features...\n",
      "Created 5692 features\n",
      "\n",
      "Tuning neural network hyperparameters...\n",
      "Tuning neural network hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:06<00:00, 185.70it/s]\n",
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:06<00:00, 184.21it/s]\n",
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:06<00:00, 184.90it/s]\n",
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:06<00:00, 186.18it/s]\n",
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:06<00:00, 185.26it/s]\n",
      "Testing NN parameters:  10%|█         | 1/10 [00:37<05:39, 37.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best parameters found:\n",
      "Parameters: {'hidden_dims': [256, 128, 64], 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 16}\n",
      "Validation Loss: 0.3253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 603/603 [00:03<00:00, 169.74it/s]\n",
      "Epoch 2/5: 100%|██████████| 603/603 [00:03<00:00, 165.68it/s]\n",
      "Epoch 3/5: 100%|██████████| 603/603 [00:03<00:00, 163.85it/s]\n",
      "Epoch 4/5: 100%|██████████| 603/603 [00:03<00:00, 161.11it/s]\n",
      "Epoch 5/5: 100%|██████████| 603/603 [00:03<00:00, 164.32it/s]\n",
      "Testing NN parameters:  20%|██        | 2/10 [01:01<03:55, 29.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best parameters found:\n",
      "Parameters: {'hidden_dims': [256, 256, 128], 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 32}\n",
      "Validation Loss: 0.3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 302/302 [00:02<00:00, 130.54it/s]\n",
      "Epoch 2/5: 100%|██████████| 302/302 [00:02<00:00, 121.05it/s]\n",
      "Epoch 3/5: 100%|██████████| 302/302 [00:02<00:00, 129.09it/s]\n",
      "Epoch 4/5: 100%|██████████| 302/302 [00:02<00:00, 130.56it/s]\n",
      "Epoch 5/5: 100%|██████████| 302/302 [00:02<00:00, 131.17it/s]\n",
      "Testing NN parameters:  30%|███       | 3/10 [01:17<02:43, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best parameters found:\n",
      "Parameters: {'hidden_dims': [256, 128, 64], 'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Validation Loss: 0.2904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:06<00:00, 180.41it/s]\n",
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:06<00:00, 185.58it/s]\n",
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:06<00:00, 184.14it/s]\n",
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:06<00:00, 182.95it/s]\n",
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:06<00:00, 187.98it/s]\n",
      "Epoch 1/5: 100%|██████████| 603/603 [00:03<00:00, 164.70it/s].07s/it]\n",
      "Epoch 2/5: 100%|██████████| 603/603 [00:03<00:00, 165.07it/s]\n",
      "Epoch 3/5: 100%|██████████| 603/603 [00:03<00:00, 166.73it/s]\n",
      "Epoch 4/5: 100%|██████████| 603/603 [00:03<00:00, 166.49it/s]\n",
      "Epoch 5/5: 100%|██████████| 603/603 [00:03<00:00, 165.08it/s]\n",
      "Testing NN parameters:  50%|█████     | 5/10 [02:18<02:14, 26.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best parameters found:\n",
      "Parameters: {'hidden_dims': [256, 128, 64], 'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 32}\n",
      "Validation Loss: 0.2889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 603/603 [00:04<00:00, 130.29it/s]\n",
      "Epoch 2/5: 100%|██████████| 603/603 [00:04<00:00, 149.34it/s]\n",
      "Epoch 3/5: 100%|██████████| 603/603 [00:04<00:00, 149.88it/s]\n",
      "Epoch 4/5: 100%|██████████| 603/603 [00:04<00:00, 143.49it/s]\n",
      "Epoch 5/5: 100%|██████████| 603/603 [00:03<00:00, 156.68it/s]\n",
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:07<00:00, 170.99it/s]9s/it]\n",
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:06<00:00, 183.36it/s]\n",
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:06<00:00, 182.40it/s]\n",
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:06<00:00, 175.64it/s]\n",
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:06<00:00, 180.55it/s]\n",
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:06<00:00, 181.01it/s]2s/it]\n",
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:06<00:00, 186.26it/s]\n",
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:06<00:00, 173.07it/s]\n",
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:07<00:00, 163.60it/s]\n",
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:06<00:00, 180.67it/s]\n",
      "Epoch 1/5: 100%|██████████| 1205/1205 [00:07<00:00, 162.44it/s]0s/it]\n",
      "Epoch 2/5: 100%|██████████| 1205/1205 [00:07<00:00, 161.11it/s]\n",
      "Epoch 3/5: 100%|██████████| 1205/1205 [00:07<00:00, 159.87it/s]\n",
      "Epoch 4/5: 100%|██████████| 1205/1205 [00:06<00:00, 174.94it/s]\n",
      "Epoch 5/5: 100%|██████████| 1205/1205 [00:06<00:00, 174.04it/s]\n",
      "Epoch 1/5: 100%|██████████| 302/302 [00:02<00:00, 123.15it/s].85s/it]\n",
      "Epoch 2/5: 100%|██████████| 302/302 [00:02<00:00, 116.50it/s]\n",
      "Epoch 3/5: 100%|██████████| 302/302 [00:02<00:00, 122.65it/s]\n",
      "Epoch 4/5: 100%|██████████| 302/302 [00:02<00:00, 131.77it/s]\n",
      "Epoch 5/5: 100%|██████████| 302/302 [00:02<00:00, 111.85it/s]\n",
      "Testing NN parameters: 100%|██████████| 10/10 [05:05<00:00, 30.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best neural network parameters: {'hidden_dims': [256, 128, 64], 'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 32}\n",
      "\n",
      "Training final model with best parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 603/603 [00:04<00:00, 136.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Average training loss: 0.3513\n",
      "Average validation loss: 0.2882\n",
      "RMSE: $22,844.13\n",
      "MAE: $17,271.08\n",
      "R2 Score: 0.7059\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 603/603 [00:04<00:00, 137.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Average training loss: 0.2307\n",
      "Average validation loss: 0.2819\n",
      "RMSE: $22,605.23\n",
      "MAE: $16,241.75\n",
      "R2 Score: 0.7120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 603/603 [00:03<00:00, 153.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Average training loss: 0.1736\n",
      "Average validation loss: 0.2963\n",
      "RMSE: $23,170.63\n",
      "MAE: $16,442.15\n",
      "R2 Score: 0.6975\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 603/603 [00:03<00:00, 160.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Average training loss: 0.1483\n",
      "Average validation loss: 0.2827\n",
      "RMSE: $22,634.44\n",
      "MAE: $16,246.93\n",
      "R2 Score: 0.7113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 603/603 [00:03<00:00, 168.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Average training loss: 0.1362\n",
      "Average validation loss: 0.2838\n",
      "RMSE: $22,674.00\n",
      "MAE: $15,865.74\n",
      "R2 Score: 0.7103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 603/603 [00:03<00:00, 158.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Average training loss: 0.1229\n",
      "Average validation loss: 0.2900\n",
      "RMSE: $22,917.12\n",
      "MAE: $16,805.92\n",
      "R2 Score: 0.7040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 603/603 [00:03<00:00, 161.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Average training loss: 0.0992\n",
      "Average validation loss: 0.2595\n",
      "RMSE: $21,686.96\n",
      "MAE: $15,330.32\n",
      "R2 Score: 0.7350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 603/603 [00:03<00:00, 166.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Average training loss: 0.0869\n",
      "Average validation loss: 0.2670\n",
      "RMSE: $21,989.79\n",
      "MAE: $15,804.66\n",
      "R2 Score: 0.7275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 603/603 [00:03<00:00, 163.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Average training loss: 0.0849\n",
      "Average validation loss: 0.2658\n",
      "RMSE: $21,948.12\n",
      "MAE: $15,378.99\n",
      "R2 Score: 0.7285\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 603/603 [00:03<00:00, 169.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Average training loss: 0.0811\n",
      "Average validation loss: 0.2661\n",
      "RMSE: $21,963.55\n",
      "MAE: $15,264.52\n",
      "R2 Score: 0.7282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 603/603 [00:03<00:00, 167.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "Average training loss: 0.0873\n",
      "Average validation loss: 0.2728\n",
      "RMSE: $22,239.07\n",
      "MAE: $15,550.90\n",
      "R2 Score: 0.7213\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 603/603 [00:03<00:00, 168.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "Average training loss: 0.0751\n",
      "Average validation loss: 0.2642\n",
      "RMSE: $21,882.62\n",
      "MAE: $15,603.60\n",
      "R2 Score: 0.7302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 603/603 [00:03<00:00, 161.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "Average training loss: 0.0669\n",
      "Average validation loss: 0.2610\n",
      "RMSE: $21,749.36\n",
      "MAE: $14,950.20\n",
      "R2 Score: 0.7334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 603/603 [00:03<00:00, 157.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "Average training loss: 0.0646\n",
      "Average validation loss: 0.2624\n",
      "RMSE: $21,804.11\n",
      "MAE: $15,502.49\n",
      "R2 Score: 0.7321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 603/603 [00:03<00:00, 153.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "Average training loss: 0.0661\n",
      "Average validation loss: 0.2662\n",
      "RMSE: $21,961.15\n",
      "MAE: $15,197.88\n",
      "R2 Score: 0.7282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 603/603 [00:03<00:00, 165.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "Average training loss: 0.0628\n",
      "Average validation loss: 0.2645\n",
      "RMSE: $21,895.74\n",
      "MAE: $15,014.50\n",
      "R2 Score: 0.7298\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 603/603 [00:03<00:00, 162.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "Average training loss: 0.0624\n",
      "Average validation loss: 0.2573\n",
      "RMSE: $21,592.36\n",
      "MAE: $14,960.95\n",
      "R2 Score: 0.7373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 603/603 [00:03<00:00, 162.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "Average training loss: 0.0591\n",
      "Average validation loss: 0.2635\n",
      "RMSE: $21,851.43\n",
      "MAE: $15,540.71\n",
      "R2 Score: 0.7309\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 603/603 [00:04<00:00, 149.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "Average training loss: 0.0572\n",
      "Average validation loss: 0.2623\n",
      "RMSE: $21,803.67\n",
      "MAE: $14,768.12\n",
      "R2 Score: 0.7321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 603/603 [00:03<00:00, 160.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "Average training loss: 0.0565\n",
      "Average validation loss: 0.2621\n",
      "RMSE: $21,797.11\n",
      "MAE: $15,314.66\n",
      "R2 Score: 0.7323\n",
      "\n",
      "\n",
      "Final Model Performance:\n",
      "RMSE: $21,797.11\n",
      "MAE: $15,314.66\n",
      "R2 Score: 0.7323\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from itertools import product\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SalaryDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets) if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.features[idx], self.targets[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "class SalaryNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout_rate=0.1):\n",
    "        super(SalaryNeuralNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "class SalaryPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.general_tfidf = None\n",
    "        self.skill_tfidf = None\n",
    "        self.neural_net = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.salary_scaler = StandardScaler()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Best parameters storage\n",
    "        self.best_skill_tfidf_params = None\n",
    "        self.best_nn_params = None\n",
    "\n",
    "    def prepare_data(self, feature_matrix, target, batch_size=16):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(feature_matrix)\n",
    "        y_scaled = self.salary_scaler.fit_transform(target.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = SalaryDataset(X_scaled, y_scaled)\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def create_features(self, df, skill_categories):\n",
    "        \"\"\"Create TF-IDF and other features\"\"\"\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "        \n",
    "        # General TF-IDF with best parameters\n",
    "        self.general_tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=0.01,\n",
    "            max_df=0.9\n",
    "        )\n",
    "        general_matrix = self.general_tfidf.fit_transform(combined_text)\n",
    "        general_features = pd.DataFrame(\n",
    "            general_matrix.toarray(),\n",
    "            columns=[f'general_{f}' for f in self.general_tfidf.get_feature_names_out()]\n",
    "        )\n",
    "        \n",
    "        # Prepare skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "        \n",
    "        # Validate skill vocabulary against text\n",
    "        valid_skill_terms = set()\n",
    "        for term in skill_vocabulary:\n",
    "            if any(term.lower() in text.lower() for text in combined_text):\n",
    "                valid_skill_terms.add(term)\n",
    "        \n",
    "        if not valid_skill_terms:\n",
    "            print(\"Warning: No skill terms found in text. Using general features only.\")\n",
    "            feature_df = general_features\n",
    "        else:\n",
    "            # Use tuned parameters if available for skill TF-IDF\n",
    "            skill_params = self.best_skill_tfidf_params or {\n",
    "                'ngram_range': (1, 2),\n",
    "                'min_df': 0.01,\n",
    "                'max_df': 0.9\n",
    "            }\n",
    "            \n",
    "            # Skill-specific TF-IDF\n",
    "            self.skill_tfidf = TfidfVectorizer(\n",
    "                vocabulary=list(valid_skill_terms),\n",
    "                stop_words=\"english\",\n",
    "                **skill_params\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                skill_matrix = self.skill_tfidf.fit_transform(combined_text)\n",
    "                skill_features = pd.DataFrame(\n",
    "                    skill_matrix.toarray(),\n",
    "                    columns=[f'skill_{f}' for f in self.skill_tfidf.get_feature_names_out()]\n",
    "                )\n",
    "                \n",
    "                # Combine features\n",
    "                feature_df = pd.concat([\n",
    "                    general_features,\n",
    "                    skill_features\n",
    "                ], axis=1)\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Error in skill TF-IDF creation: {str(e)}\")\n",
    "                print(\"Proceeding with general features only.\")\n",
    "                feature_df = general_features\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        feature_df['work_type_encoded'] = self.label_encoder.fit_transform(df['work_type'].fillna('UNKNOWN'))\n",
    "        feature_df['company_encoded'] = self.label_encoder.fit_transform(df['company'].fillna('UNKNOWN'))\n",
    "        feature_df['industry_encoded'] = self.label_encoder.fit_transform(df['industry'].fillna('UNKNOWN'))\n",
    "        \n",
    "        print(f\"Created {feature_df.shape[1]} features\")\n",
    "        return feature_df\n",
    "\n",
    "    def tune_neural_network(self, feature_matrix, target, max_trials=10):\n",
    "        \"\"\"Tune neural network hyperparameters\"\"\"\n",
    "        print(\"Tuning neural network hyperparameters...\")\n",
    "        \n",
    "        nn_param_grid = {\n",
    "            'hidden_dims': [\n",
    "                [256, 128, 64],\n",
    "                [512, 256, 128],\n",
    "                [256, 256, 128],\n",
    "                [512, 256, 128, 64]\n",
    "            ],\n",
    "            'dropout_rate': [0.1, 0.2, 0.3],\n",
    "            'learning_rate': [1e-4, 2e-4, 5e-4],\n",
    "            'batch_size': [16, 32, 64]\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        param_combinations = [dict(zip(nn_param_grid.keys(), v)) \n",
    "                            for v in product(*nn_param_grid.values())]\n",
    "        \n",
    "        # Randomly sample from parameter combinations if there are too many\n",
    "        if len(param_combinations) > max_trials:\n",
    "            param_combinations = np.random.choice(\n",
    "                param_combinations, \n",
    "                size=max_trials, \n",
    "                replace=False\n",
    "            )\n",
    "        \n",
    "        for params in tqdm(param_combinations, desc=\"Testing NN parameters\"):\n",
    "            try:\n",
    "                # Prepare data with current batch size\n",
    "                train_loader, val_loader = self.prepare_data(\n",
    "                    feature_matrix, \n",
    "                    target, \n",
    "                    batch_size=params['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Initialize model with current parameters\n",
    "                self.neural_net = SalaryNeuralNet(\n",
    "                    input_dim=feature_matrix.shape[1],\n",
    "                    hidden_dims=params['hidden_dims'],\n",
    "                    dropout_rate=params['dropout_rate']\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Train for a few epochs to evaluate parameters\n",
    "                results = self.train_model(\n",
    "                    feature_matrix,\n",
    "                    target,\n",
    "                    epochs=5,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    is_tuning=True\n",
    "                )\n",
    "                \n",
    "                val_loss = min(results['training_history']['val_loss'])\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = params\n",
    "                    print(f\"\\nNew best parameters found:\")\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_nn_params = best_params\n",
    "        return best_params\n",
    "\n",
    "    def train_model(self, feature_matrix, target, epochs=10, batch_size=None, \n",
    "                   learning_rate=None, is_tuning=False):\n",
    "        \"\"\"Train neural network model\"\"\"\n",
    "        # Use default parameters if none provided and not in tuning mode\n",
    "        if not is_tuning:\n",
    "            if self.best_nn_params:\n",
    "                batch_size = batch_size or self.best_nn_params.get('batch_size', 16)\n",
    "                learning_rate = learning_rate or self.best_nn_params.get('learning_rate', 2e-5)\n",
    "            else:\n",
    "                batch_size = batch_size or 16\n",
    "                learning_rate = learning_rate or 2e-5\n",
    "        \n",
    "        # Prepare data\n",
    "        train_loader, val_loader = self.prepare_data(feature_matrix, target, batch_size)\n",
    "        \n",
    "        # Initialize model if not already initialized\n",
    "        if not self.neural_net or is_tuning:\n",
    "            hidden_dims = self.best_nn_params.get('hidden_dims', [256, 128, 64]) if self.best_nn_params else [256, 128, 64]\n",
    "            dropout_rate = self.best_nn_params.get('dropout_rate', 0.1) if self.best_nn_params else 0.1\n",
    "            \n",
    "            self.neural_net = SalaryNeuralNet(\n",
    "                input_dim=feature_matrix.shape[1],\n",
    "                hidden_dims=hidden_dims,\n",
    "                dropout_rate=dropout_rate\n",
    "            ).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.neural_net.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        training_history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.neural_net.train()\n",
    "            train_loss = 0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for features, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "                features = features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.neural_net(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            training_history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.neural_net.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            val_preds = []\n",
    "            val_true = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    \n",
    "                    outputs = self.neural_net(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    val_preds.extend(outputs.cpu().numpy())\n",
    "                    val_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            training_history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            if not is_tuning:\n",
    "                # Transform predictions back to original scale\n",
    "                val_preds = self.salary_scaler.inverse_transform(np.array(val_preds).reshape(-1, 1))\n",
    "                val_true = self.salary_scaler.inverse_transform(np.array(val_true).reshape(-1, 1))\n",
    "                \n",
    "                # Calculate metrics\n",
    "                rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
    "                mae = mean_absolute_error(val_true, val_preds)\n",
    "                r2 = r2_score(val_true, val_preds)\n",
    "                \n",
    "                print(f'Epoch {epoch + 1}:')\n",
    "                print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "                print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "                print(f'RMSE: ${rmse:,.2f}')\n",
    "                print(f'MAE: ${mae:,.2f}')\n",
    "                print(f'R2 Score: {r2:.4f}\\n')\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                if not is_tuning:\n",
    "                    torch.save(self.neural_net.state_dict(), 'best_salary_predictor.pth')\n",
    "        \n",
    "        if not is_tuning:\n",
    "            final_metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            }\n",
    "        else:\n",
    "            final_metrics = {}\n",
    "        \n",
    "        return {\n",
    "            'training_history': training_history,\n",
    "            'final_metrics': final_metrics\n",
    "        }\n",
    "\n",
    "    def save_model(self, path_prefix='models/'):\n",
    "        \"\"\"Save all model components\"\"\"\n",
    "        import os\n",
    "        os.makedirs(path_prefix, exist_ok=True)\n",
    "        \n",
    "        joblib.dump(self.best_nn_params, f'{path_prefix}best_nn_params.joblib')\n",
    "        torch.save(self.neural_net.state_dict(), f'{path_prefix}neural_net.pt')\n",
    "\n",
    "    def load_model(self, path_prefix='models/'):\n",
    "        \"\"\"Load all model components\"\"\"\n",
    "        self.general_tfidf = joblib.load(f'{path_prefix}general_tfidf.joblib')\n",
    "        self.skill_tfidf = joblib.load(f'{path_prefix}skill_tfidf.joblib')\n",
    "        self.scaler = joblib.load(f'{path_prefix}scaler.joblib')\n",
    "        self.label_encoder = joblib.load(f'{path_prefix}label_encoder.joblib')\n",
    "        self.salary_scaler = joblib.load(f'{path_prefix}salary_scaler.joblib')\n",
    "        self.best_skill_tfidf_params = joblib.load(f'{path_prefix}best_skill_tfidf_params.joblib')\n",
    "        self.best_nn_params = joblib.load(f'{path_prefix}best_nn_params.joblib')\n",
    "        \n",
    "        # Initialize neural network with best parameters\n",
    "        self.neural_net = SalaryNeuralNet(\n",
    "            input_dim=self.scaler.n_features_in_,\n",
    "            hidden_dims=self.best_nn_params['hidden_dims'],\n",
    "            dropout_rate=self.best_nn_params['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        self.neural_net.load_state_dict(torch.load(f'{path_prefix}neural_net.pt'))\n",
    "        self.neural_net.eval()\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        # Create features\n",
    "        features = self.create_features(df)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.neural_net(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # Scale back predictions\n",
    "        predictions = self.salary_scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "        \n",
    "        return predictions.flatten()\n",
    "\n",
    "    def tune_skill_tfidf(self, df, skill_categories):\n",
    "        \"\"\"Tune skill-specific TF-IDF parameters\"\"\"\n",
    "        print(\"Tuning skill-specific TF-IDF parameters...\")\n",
    "        combined_text = df['cleaned_title'] + ' ' + df['cleaned_description']\n",
    "    \n",
    "        # Skill-specific parameter grid\n",
    "        skill_param_grid = {\n",
    "            'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "            'min_df': [0.005, 0.01, 0.02],  # Lower values for skill terms\n",
    "            'max_df': [0.8, 0.9, 0.95]\n",
    "        }\n",
    "    \n",
    "        best_skill_score = float('-inf')\n",
    "        best_skill_params = None\n",
    "        best_feature_stats = None\n",
    "    \n",
    "        # Create skill vocabulary\n",
    "        skill_vocabulary = set()\n",
    "        for category in skill_categories.values():\n",
    "            skill_vocabulary.update(category['keywords'])\n",
    "            skill_vocabulary.update(category['context_required'])\n",
    "    \n",
    "        # Test different parameter combinations\n",
    "        param_combinations = [dict(zip(skill_param_grid.keys(), v)) \n",
    "                            for v in product(*skill_param_grid.values())]\n",
    "    \n",
    "        for params in tqdm(param_combinations, desc=\"Testing skill TF-IDF parameters\"):\n",
    "            try:\n",
    "                skill_tfidf = TfidfVectorizer(\n",
    "                    vocabulary=list(skill_vocabulary),\n",
    "                    stop_words=\"english\",\n",
    "                    **params\n",
    "                )\n",
    "            \n",
    "                feature_matrix = skill_tfidf.fit_transform(combined_text)\n",
    "                n_features = feature_matrix.shape[1]\n",
    "                sparsity = 1.0 - (feature_matrix.nnz / (feature_matrix.shape[0] * feature_matrix.shape[1]))\n",
    "            \n",
    "                # Calculate feature frequency statistics\n",
    "                feature_frequencies = np.asarray(feature_matrix.sum(axis=0)).ravel()\n",
    "                avg_freq = np.mean(feature_frequencies)\n",
    "                max_freq = np.max(feature_frequencies)\n",
    "            \n",
    "                # Score based on feature coverage and sparsity\n",
    "                score = n_features * (1 - sparsity) * avg_freq\n",
    "            \n",
    "                if score > best_skill_score:\n",
    "                    best_skill_score = score\n",
    "                    best_skill_params = params\n",
    "                    best_feature_stats = {\n",
    "                        'n_features': n_features,\n",
    "                        'sparsity': sparsity,\n",
    "                        'avg_frequency': avg_freq,\n",
    "                        'max_frequency': max_freq\n",
    "                    }\n",
    "                    print(f\"\\nNew best skill parameters found:\")\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Features: {n_features}\")\n",
    "                    print(f\"Sparsity: {sparsity:.4f}\")\n",
    "                    print(f\"Average Frequency: {avg_freq:.4f}\")\n",
    "                    print(f\"Score: {score:.4f}\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "        self.best_skill_tfidf_params = best_skill_params\n",
    "        return best_skill_params, best_feature_stats\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    skill_categories = {\n",
    "            'Creative Arts': {\n",
    "                'keywords': [\n",
    "                    'adobe creative suite', 'photoshop', 'illustrator', 'indesign',\n",
    "                    'graphic design', 'visual design', 'typography', 'art direction',\n",
    "                    'creative direction', 'brand design', 'illustration', 'adobe xd',\n",
    "                    'figma', 'sketch', 'color theory', 'layout design'\n",
    "                ],\n",
    "                'context_required': ['design', 'creative', 'art', 'visual']\n",
    "            },\n",
    "            'Digital Design': {\n",
    "                'keywords': [\n",
    "                    'ui design', 'ux design', 'user interface', 'user experience',\n",
    "                    'wireframing', 'prototyping', 'responsive design', 'mobile design',\n",
    "                    'web design', 'interaction design', 'usability testing',\n",
    "                    'information architecture', 'figma', 'sketch', 'principle'\n",
    "                ],\n",
    "                'context_required': ['design', 'user', 'interface', 'experience']\n",
    "            },\n",
    "            'Product Management': {\n",
    "                'keywords': [\n",
    "                    'product strategy', 'product roadmap', 'product development',\n",
    "                    'product lifecycle', 'agile product', 'product owner', 'scrum',\n",
    "                    'market research', 'user stories', 'feature prioritization',\n",
    "                    'product metrics', 'product analytics', 'product launch'\n",
    "                ],\n",
    "                'context_required': ['product']\n",
    "            },\n",
    "            'Data Analysis': {\n",
    "                'keywords': [\n",
    "                    'data analysis', 'statistical analysis', 'data visualization',\n",
    "                    'sql', 'python', 'r programming', 'tableau', 'power bi',\n",
    "                    'excel advanced', 'data modeling', 'regression analysis',\n",
    "                    'hypothesis testing', 'a/b testing', 'data mining'\n",
    "                ],\n",
    "                'context_required': ['data', 'analysis', 'analytics']\n",
    "            },\n",
    "            'Software Development': {\n",
    "                'keywords': [\n",
    "                    'java ', 'python', 'javascript', 'react', 'angular', 'node.js',\n",
    "                    'full stack', 'front end', 'back end', 'web development',\n",
    "                    'api development', 'cloud computing', 'aws', 'azure',\n",
    "                    'devops', 'ci/cd', 'docker', 'kubernetes'\n",
    "                ],\n",
    "                'context_required': ['development', 'programming', 'software']\n",
    "            },\n",
    "            'Marketing': {\n",
    "                'keywords': [\n",
    "                    'digital marketing', 'content marketing', 'seo', 'sem',\n",
    "                    'social media marketing', 'email marketing', 'marketing automation',\n",
    "                    'google analytics', 'conversion optimization', 'brand marketing',\n",
    "                    'marketing strategy', 'campaign management', 'hubspot', 'marketo'\n",
    "                ],\n",
    "                'context_required': ['marketing', 'digital']\n",
    "            },\n",
    "            'Project Management': {\n",
    "                'keywords': [\n",
    "                    'project management', 'agile methodology', 'scrum master',\n",
    "                    'project planning', 'risk management', 'stakeholder management',\n",
    "                    'pmp certification', 'project coordination', 'jira', 'asana',\n",
    "                    'microsoft project', 'project lifecycle', 'change management'\n",
    "                ],\n",
    "                'context_required': ['project', 'management']\n",
    "            },\n",
    "            'Business Analysis': {\n",
    "                'keywords': [\n",
    "                    'business analysis', 'requirements gathering', 'process mapping',\n",
    "                    'gap analysis', 'business process', 'system analysis',\n",
    "                    'functional requirements', 'business intelligence', 'data modeling',\n",
    "                    'process improvement', 'workflow optimization'\n",
    "                ],\n",
    "                'context_required': ['analysis', 'business']\n",
    "            },\n",
    "            'Financial': {\n",
    "                'keywords': [\n",
    "                    'financial analysis', 'financial modeling', 'forecasting',\n",
    "                    'budgeting', 'variance analysis', 'cost analysis', 'pricing',\n",
    "                    'profit and loss', 'balance sheet', 'financial reporting',\n",
    "                    'risk assessment', 'investment analysis'\n",
    "                ],\n",
    "                'context_required': ['financial', 'finance']\n",
    "            },\n",
    "            'Sales': {\n",
    "                'keywords': [\n",
    "                    'sales strategy', 'account management', 'sales forecasting',\n",
    "                    'crm', 'salesforce', 'sales operations', 'business development',\n",
    "                    'lead generation', 'pipeline management', 'contract negotiation',\n",
    "                    'sales analytics', 'territory management'\n",
    "                ],\n",
    "                'context_required': ['sales', 'revenue']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Initialize model\n",
    "    model = SalaryPredictionModel()\n",
    "    \n",
    "    # Tune skill-specific TF-IDF parameters\n",
    "    print(\"\\nTuning skill-specific TF-IDF parameters...\")\n",
    "    best_skill_params, skill_stats = model.tune_skill_tfidf(df, skill_categories)\n",
    "    \n",
    "    print(\"\\nBest Skill TF-IDF Parameters:\")\n",
    "    print(f\"N-gram Range: {best_skill_params['ngram_range']}\")\n",
    "    print(f\"Min Document Frequency: {best_skill_params['min_df']}\")\n",
    "    print(f\"Max Document Frequency: {best_skill_params['max_df']}\")\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(f\"Number of Features: {skill_stats['n_features']}\")\n",
    "    print(f\"Sparsity: {skill_stats['sparsity']:.4f}\")\n",
    "    print(f\"Average Feature Frequency: {skill_stats['avg_frequency']:.4f}\")\n",
    "    print(f\"Maximum Feature Frequency: {skill_stats['max_frequency']:.4f}\")\n",
    "    \n",
    "    # Create features using tuned parameters\n",
    "    print(\"\\nCreating features...\")\n",
    "    feature_matrix = model.create_features(df, skill_categories)\n",
    "    \n",
    "    # Tune neural network hyperparameters\n",
    "    print(\"\\nTuning neural network hyperparameters...\")\n",
    "    nn_params = model.tune_neural_network(feature_matrix, df['salary'], max_trials=10)\n",
    "    print(\"\\nBest neural network parameters:\", nn_params)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    results = model.train_model(\n",
    "        feature_matrix,\n",
    "        df['salary'],\n",
    "        epochs=20\n",
    "    )\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    print(f\"RMSE: ${results['final_metrics']['RMSE']:,.2f}\")\n",
    "    print(f\"MAE: ${results['final_metrics']['MAE']:,.2f}\")\n",
    "    print(f\"R2 Score: {results['final_metrics']['R2']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving model...\")\n",
    "    model.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
